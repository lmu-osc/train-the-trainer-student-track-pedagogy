@article{abele-brehmWerSollProfessur2016,
  title = {Wer Soll Die {{Professur}} Bekommen?},
  author = {{Abele-Brehm}, Andrea E. and B{\"u}hner, Markus},
  year = 2016,
  journal = {Psychologische Rundschau},
  volume = {67},
  number = {4},
  pages = {250--261},
  publisher = {Hogrefe Verlag},
  issn = {0033-3042},
  doi = {10.1026/0033-3042/a000335},
  urldate = {2025-10-30},
  abstract = {Zusammenfassung. Die Entwicklung einer Wissenschaft ist abh\"angig von den Personen, die sie tragen. Der Auswahl geeigneter Personen in Berufungsverfahren auf Professuren kommt deshalb eine besondere Bedeutung zu. Die vorliegende Studie besch\"aftigt sich erstmals damit, wie Kolleginnen und Kollegen der Psychologie Berufungsverfahren beurteilen; wie wichtig sie verschiedene Indikatoren f\"ur die Eignung auf eine Professur einsch\"atzen; wie hoch die Diskrepanz zwischen gew\"unschter und tats\"achlicher Relevanz dieser Indikatoren ist; sowie wie sie zu verschiedenen Ausgestaltungsm\"oglichkeiten von Berufungsverfahren stehen. Es wurden 3.784~Mitglieder der DGPs angeschrieben, um an einer online Befragung teilzunehmen. N~=~1.453 Personen beantworteten zumindest einen Teil der Fragen. Die Ergebnisse zeigen, dass die Diskrepanzen zwischen Ist und Soll bei \"uberfachlichen Kompetenzen (Kommunikation, Kooperation, strategisches Denken) besonders gro\ss{} sind und dass die Befragten den Stellenwert quantitativer Forschungsleistungsindikatoren als zu hoch ansehen. Die Befragten bef\"urworten den Einsatz strukturierter Interviews zur Erfassung \"uberfachlicher Kompetenzen, eine multi-methodale Messung der Forschungs- und Lehrleistung durch qualitative und quantitative Indikatoren sowie st\"arker strukturierte Probelehrvortr\"age. M\"ogliche fachpolitische Konsequenzen dieser Befunde werden diskutiert.},
  keywords = {Berufungsverfahren,Eignungskriterien,hiring procedures for professors,indicators of suitability,quantitative and qualitative research performance,quantitative und qualitative Indikatoren der Forschungsleistung,social and management skills,soziale und Management Fertigkeiten},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\UXK4GL8S\Abele-Brehm and Bühner - 2016 - Wer soll die Professur bekommen.pdf}
}

@article{alkaissiArtificialHallucinationsChatGPT2023,
  title = {Artificial {{Hallucinations}} in {{ChatGPT}}: {{Implications}} in {{Scientific Writing}}},
  shorttitle = {Artificial {{Hallucinations}} in {{ChatGPT}}},
  author = {Alkaissi, Hussam and McFarlane, Samy I},
  year = 2023,
  journal = {Cureus},
  issn = {2168-8184},
  doi = {10.7759/cureus.35179},
  urldate = {2025-12-10},
  abstract = {While still in its infancy, ChatGPT (Generative Pretrained Transformer), introduced in November 2022, is bound to hugely impact many industries, including healthcare, medical education, biomedical research, and scientific writing. Implications of ChatGPT, that new chatbot introduced by OpenAI on academic writing, is largely unknown. In response to the Journal of Medical Science (Cureus) Turing Test - call for case reports written with the assistance of ChatGPT, we present two cases one of homocystinuria-associated osteoporosis, and the other is on late-onset Pompe disease (LOPD), a rare metabolic disorder. We tested ChatGPT to write about the pathogenesis of these conditions. We documented the positive, negative, and rather troubling aspects of our newly introduced chatbot's performance.},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\MTN9JSAX\Alkaissi and McFarlane - 2023 - Artificial Hallucinations in ChatGPT Implications in Scientific Writing.pdf}
}

@book{andersonTaxonomyLearningTeaching2001,
  title = {A Taxonomy for Learning, Teaching, and Assessing : A Revision of {{Bloom}}'s Taxonomy of Educational Objectives : Complete Edition},
  shorttitle = {A Taxonomy for Learning, Teaching, and Assessing},
  author = {Anderson, Lorin W. and Krathwohl, David R.},
  year = 2001,
  publisher = {Addison Wesley Longman, Inc.},
  urldate = {2025-12-18},
  isbn = {978-0-321-08405-7},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\JYBQVKMT\18824.html}
}

@article{armitageRepeatedSignificanceTests1969,
  title = {Repeated Significance Tests on Accumulating Data},
  author = {Armitage, P. and McPherson, C. K. and Rowe, B. C.},
  year = 1969,
  journal = {Journal of the Royal Statistical Society: Series A (General)},
  volume = {132},
  number = {2},
  pages = {235--244},
  issn = {2397-2327},
  doi = {10.2307/2343787},
  urldate = {2025-10-30},
  abstract = {If significance tests at a fixed level are repeated at stages during the accumulation of data the probability of obtaining a significant result when the null hypothesis is true rises above the nominal significance level. Numerical results are presented for repeated tests on cumulative series of binomial, normal and exponential observations.},
  copyright = {\copyright{} 1969 The Authors},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\SG73L9N6\Armitage et al. - 1969 - Repeated Significance Tests on Accumulating Data.pdf}
}

@article{bainesAdviceArtificialIntelligence2024,
  title = {Advice from Artificial Intelligence: A Review and Practical Implications},
  shorttitle = {Advice from Artificial Intelligence},
  author = {Baines, Julia I. and Dalal, Reeshad S. and Ponce, Lida P. and Tsai, Ho-Chun},
  year = 2024,
  journal = {Frontiers in Psychology},
  volume = {15},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2024.1390182},
  urldate = {2025-12-10},
  abstract = {Despite considerable behavioral and organizational research on advice from human advisors, and despite the increasing study of artificial intelligence (AI) in organizational research, workplace-related applications, and popular discourse, an interdisciplinary review of advice from AI (vs. human) advisors has yet to be undertaken. We argue that the increasing adoption of AI to augment human decision-making would benefit from a framework that can characterize such interactions. Thus, the current research invokes judgment and decision-making research on advice from human advisors and uses a conceptual ``fit''-based model to: (1) summarize how the characteristics of the AI advisor, human decision-maker, and advice environment influence advice exchanges and outcomes (including informed speculation about the durability of such findings in light of rapid advances in AI technology), (2) delineate future research directions (along with specific predictions), and (3) provide practical implications involving the use of AI advice by human decision-makers in applied settings.},
  langid = {english},
  keywords = {advice,ADVISOR,algorithm,Anthropomorphize,artificial intelligence,Chatbot,Robo-advisor,Virtual assistant},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\4AME5UA2\Baines et al. - 2024 - Advice from artificial intelligence a review and practical implications.pdf}
}

@article{baker1500ScientistsLift2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  author = {Baker, Monya},
  year = 2016,
  month = may,
  journal = {Nature},
  volume = {533},
  number = {7604},
  pages = {452--454},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/533452a},
  urldate = {2025-11-06},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\QCWC283L\Baker - 2016 - 1,500 scientists lift the lid on reproducibility.pdf}
}

@article{begleyRaiseStandardsPreclinical2012,
  title = {Raise Standards for Preclinical Cancer Research},
  author = {Begley, C. Glenn and Ellis, Lee M.},
  year = 2012,
  journal = {Nature},
  volume = {483},
  number = {7391},
  pages = {531--533},
  issn = {1476-4687},
  doi = {10.1038/483531a},
  abstract = {C. Glenn Begley and Lee M. Ellis propose how methods, publications and incentives must change if patients are to benefit.},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\PNJJSSP7\Begley and Ellis - 2012 - Raise standards for preclinical cancer research.pdf}
}

@misc{bergmaBuffetApproachOpen2023,
  title = {The {{Buffet Approach}} to {{Open Science}}},
  author = {Bergma, Christina},
  year = 2023
}

@article{bloom1955NormativeStudy1956,
  title = {The 1955 {{Normative Study}} of the {{Tests}} of {{General Educational Development}}},
  author = {Bloom, B. S.},
  year = 1956,
  journal = {The School Review},
  volume = {64},
  number = {3},
  pages = {110--124},
  publisher = {The University of Chicago Press},
  issn = {0036-6773},
  doi = {10.1086/442296},
  urldate = {2025-12-18}
}

@article{boucherArtificiallyIntelligentChatbots2021,
  title = {Artificially Intelligent Chatbots in Digital Mental Health Interventions: A Review},
  shorttitle = {Artificially Intelligent Chatbots in Digital Mental Health Interventions},
  author = {Boucher, Eliane M. and Harake, Nicole R. and Ward, Haley E. and Stoeckl, Sarah Elizabeth and Vargas, Junielly and Minkel, Jared and Parks, Acacia C. and Zilca, Ran},
  year = 2021,
  journal = {Expert Review of Medical Devices},
  volume = {18},
  number = {sup1},
  pages = {37--49},
  publisher = {Taylor \& Francis},
  issn = {1743-4440},
  doi = {10.1080/17434440.2021.2013200},
  urldate = {2025-12-09},
  abstract = {Increasing demand for mental health services and the expanding capabilities of artificial intelligence (AI) in recent years has driven the development of digital mental health interventions (DMHIs). To date, AI-based chatbots have been integrated into DMHIs to support diagnostics and screening, symptom management and behavior change, and content delivery. We summarize the current landscape of DMHIs, with a focus on AI-based chatbots. Happify Health's AI chatbot, Anna, serves as a case study for discussion of potential challenges and how these might be addressed, and demonstrates the promise of chatbots as effective, usable, and adoptable within DMHIs. Finally, we discuss ways in which future research can advance the field, addressing topics including perceptions of AI, the impact of individual differences, and implications for privacy and ethics. Our discussion concludes with a speculative viewpoint on the future of AI in DMHIs, including the use of chatbots, the evolution of AI, dynamic mental health systems, hyper-personalization, and human-like intervention delivery.},
  pmid = {34872429},
  keywords = {Artificial intelligence (AI),chatbots,digital mental health,digital mental health interventions (dmhis)},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\M6D49T3M\Boucher et al. - 2021 - Artificially intelligent chatbots in digital mental health interventions a review.pdf}
}

@article{bravemanSystemicStructuralRacism2022,
  title = {Systemic {{And Structural Racism}}: {{Definitions}}, {{Examples}}, {{Health Damages}}, {{And Approaches To Dismantling}}: {{Study}} Examines Definitions, Examples, Health Damages, and Dismantling Systemic and Structural Racism.},
  shorttitle = {Systemic {{And Structural Racism}}},
  author = {Braveman, Paula A. and Arkin, Elaine and Proctor, Dwayne and Kauh, Tina and Holm, Nicole},
  year = 2022,
  month = feb,
  journal = {Health Affairs},
  volume = {41},
  number = {2},
  pages = {171--178},
  issn = {0278-2715, 1544-5208},
  doi = {10.1377/hlthaff.2021.01394},
  urldate = {2025-09-25},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\VES28CTC\Braveman et al. - 2022 - Systemic And Structural Racism Definitions, Examples, Health Damages, And Approaches To Dismantling.pdf}
}

@article{camererEvaluatingReplicabilityLaboratory2016,
  title = {Evaluating Replicability of Laboratory Experiments in Economics},
  author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
  year = 2016,
  journal = {Science},
  volume = {351},
  number = {6280},
  pages = {1433--1436},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aaf0918},
  urldate = {2025-11-21},
  abstract = {The replicability of some scientific findings has recently been called into question. To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. All of these replications followed predefined analysis plans that were made publicly available beforehand, and they all have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We found a significant effect in the same direction as in the original study for 11 replications (61\%); on average, the replicated effect size is 66\% of the original. The replicability rate varies between 67\% and 78\% for four additional replicability indicators, including a prediction market measure of peer beliefs.},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\M92QMYNS\Camerer et al. - 2016 - Evaluating replicability of laboratory experiments in economics.pdf}
}

@article{casuAIChatbotsMental,
  title = {{{AI Chatbots}} for {{Mental Health}}: {{A Scoping Review}} of {{Effectiveness}}, {{Feasibility}}, and {{Applications}}},
  author = {Casu, Mirko and Triscari, Sergio and Battiato, Sebastiano and Guarnera, Luca and Caponnetto, Pasquale},
  abstract = {Mental health disorders are a leading cause of disability worldwide, and there is a global shortage of mental health professionals. AI chatbots have emerged as a potential solution, offering accessible and scalable mental health interventions. This study aimed to conduct a scoping review to evaluate the effectiveness and feasibility of AI chatbots in treating mental health conditions. A literature search was conducted across multiple databases, including MEDLINE, Scopus, and PsycNet, as well as using AI-powered tools like Microsoft Copilot and Consensus. Relevant studies on AI chatbot interventions for mental health were selected based on predefined inclusion and exclusion criteria. Data extraction and quality assessment were performed independently by multiple reviewers. The search yielded 15 eligible studies covering various application areas, such as mental health support during COVID-19, interventions for specific conditions (e.g., depression, anxiety, substance use disorders), preventive care, health promotion, and usability assessments. AI chatbots demonstrated potential benefits in improving mental and emotional well-being, addressing specific mental health conditions, and facilitating behavior change. However, challenges related to usability, engagement, and integration with existing healthcare systems were identified. AI chatbots hold promise for mental health interventions, but widespread adoption hinges on improving usability, engagement, and integration with healthcare systems. Enhancing personalization and context-specific adaptation is key. Future research should focus on large-scale trials, optimal human--AI integration, and addressing ethical and social implications.},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\MU2FG33X\Casu et al. - AI Chatbots for Mental Health A Scoping Review of Effectiveness, Feasibility, and Applications.pdf}
}

@misc{changEconomicsResearchReplicable2015,
  type = {{{SSRN Scholarly Paper}}},
  title = {Is {{Economics Research Replicable}}? {{Sixty Published Papers}} from {{Thirteen Journals Say}} '{{Usually Not}}'},
  shorttitle = {Is {{Economics Research Replicable}}?},
  author = {Chang, Andrew C. and Li, Phillip},
  year = 2015,
  number = {2669564},
  eprint = {2669564},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.2669564},
  urldate = {2025-11-19},
  abstract = {We attempt to replicate 67 papers published in 13 well-regarded economics journals using author-provided replication files that include both data and code. Some journals in our sample require data and code replication files, and other journals do not require such files. Aside from 6 papers that use confidential data, we obtain data and code replication files for 29 of 35 papers (83\%) that are required to provide such files as a condition of publication, compared to 11 of 26 papers (42\%) that are not required to provide data and code replication files. We successfully replicate the key qualitative result of 22 of 67 papers (33\%) without contacting the authors. Excluding the 6 papers that use confidential data and the 2 papers that use software we do not possess, we replicate 29 of 59 papers (49\%) with assistance from the authors. Because we are able to replicate less than half of the papers in our sample even with help from the authors, we assert that economics research is usually not replicable. We conclude with recommendations on improving replication of economics research.},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {Data and Code Archives,Gross Domestic Product,Journals,Macroeconomics,National Income and Product Accounts,Replication},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\QPFRDQJE\Chang and Li - 2015 - Is Economics Research Replicable Sixty Published Papers from Thirteen Journals Say 'Usually Not'.pdf}
}

@article{chopikHowWhetherTeach2018,
  title = {How (and {{Whether}}) to Teach Undergraduates about the Replication Crisis in Psychological Science},
  author = {Chopik, William J. and Bremner, Ryan H. and Defever, Andrew M. and Keller, Victor N.},
  year = 2018,
  journal = {Teaching of Psychology},
  volume = {45},
  number = {2},
  pages = {158--163},
  publisher = {SAGE Publications Inc},
  issn = {0098-6283},
  doi = {10.1177/0098628318762900},
  urldate = {2025-11-19},
  abstract = {Over the past 10 years, crises surrounding replication, fraud, and best practices in research methods have dominated discussions in the field of psychology. However, no research exists examining how to communicate these issues to undergraduates and what effect this has on their attitudes toward the field. We developed and validated a 1-hr lecture communicating issues surrounding the replication crisis and current recommendations to increase reproducibility. Pre- and post-lecture surveys suggest that the lecture serves as an excellent pedagogical tool. Following the lecture, students trusted psychological studies slightly less but saw greater similarities between psychology and natural science fields. We discuss challenges for instructors taking the initiative to communicate these issues to undergraduates in an evenhanded way.},
  langid = {english}
}

@article{colavizzaCaseHumanitiesCitation2023,
  title = {The Case for the {{Humanities Citation Index}} ({{HuCI}}): A Citation Index by the Humanities, for the Humanities},
  shorttitle = {The Case for the {{Humanities Citation Index}} ({{HuCI}})},
  author = {Colavizza, Giovanni and Peroni, Silvio and Romanello, Matteo},
  year = 2023,
  month = dec,
  journal = {International Journal on Digital Libraries},
  volume = {24},
  number = {4},
  pages = {191--204},
  issn = {1432-5012, 1432-1300},
  doi = {10.1007/s00799-022-00327-0},
  urldate = {2025-09-25},
  abstract = {Abstract             Citation indexes are by now part of the research infrastructure in use by most scientists: a necessary tool in order to cope with the increasing amounts of scientific literature being published. Commercial citation indexes are designed for the sciences and have uneven coverage and unsatisfactory characteristics for humanities scholars, while no comprehensive citation index is published by a public organisation. We argue that an open citation index for the humanities is desirable, for four reasons: it would greatly improve and accelerate the retrieval of sources, it would offer a way to interlink collections across repositories (such as archives and libraries), it would foster the adoption of metadata standards and best practices by all stakeholders (including publishers) and it would contribute research data to fields such as bibliometrics and science studies. We also suggest that the citation index should be informed by a set of requirements relevant to the humanities. We discuss four such requirements: source coverage must be comprehensive, including books and citations to primary sources; there needs to be chronological depth, as scholarship in the humanities remains relevant over time; the index should be collection driven, leveraging the accumulated thematic collections of specialised research libraries; and it should be rich in context in order to allow for the qualification of each citation, for example, by providing citation excerpts. We detail the fit-for-purpose research infrastructure which can make the Humanities Citation Index a reality. Ultimately, we argue that a citation index for the humanities can be created by humanists, via a collaborative, distributed and open effort.},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\5NGTHZG7\Colavizza et al. - 2023 - The case for the Humanities Citation Index (HuCI) a citation index by the humanities, for the human.pdf}
}

@article{collyerGlobalPatternsPublishing2018,
  title = {Global Patterns in the Publishing of Academic Knowledge: {{Global North}}, Global {{South}}},
  shorttitle = {Global Patterns in the Publishing of Academic Knowledge},
  author = {Collyer, Fran M},
  year = 2018,
  month = jan,
  journal = {Current Sociology},
  volume = {66},
  number = {1},
  pages = {56--73},
  issn = {0011-3921, 1461-7064},
  doi = {10.1177/0011392116680020},
  urldate = {2025-09-25},
  abstract = {Much is made of the persistent structures of inequality that determine the production and distribution of goods and services across the world, but less is known about the inequalities of global academic knowledge production, and even a smaller amount about the nature of the publication industry upon which this production process depends. Reflecting on an international study of academic publishing that has been framed within the lens of Southern theory, this article explores some of the issues facing those who work and publish in the global South, and offers an analysis of several of the mechanisms that assist to maintain the inequalities of the knowledge system. The focus then moves to an examination of some recent developments in academic publishing which challenge the dominance of the global North: the building of alternative transnational circuits of publishing that provide effective pathways for the distribution of academic knowledge from `inside the global South'.},
  langid = {english}
}

@article{craigUsingRetractedJournal2020,
  title = {Using Retracted Journal Articles in Psychology to Understand Research Misconduct in the Social Sciences: {{What}} Is to Be Done?},
  shorttitle = {Using Retracted Journal Articles in Psychology to Understand Research Misconduct in the Social Sciences},
  author = {Craig, Russell and Cox, Adam and Tourish, Dennis and Thorpe, Alistair},
  year = 2020,
  journal = {Research Policy},
  volume = {49},
  number = {4},
  pages = {103930},
  issn = {0048-7333},
  doi = {10.1016/j.respol.2020.103930},
  urldate = {2025-10-30},
  abstract = {This paper explores the nature and impact of research misconduct in psychology by analyzing 160 articles that were retracted from prominent scholarly journals between 1998 and 2017. We compare findings with recent studies of retracted papers in economics, and business and management, to profile practices that are likely to be problematic in cognate social science disciplines. In psychology, the principal reason for retraction was data fabrication. Retractions took longer to make, and generally were from higher ranked and more prestigious journals, than in the two cognate disciplines. We recommend that journal editors should be more forthcoming in the reasons they provide for article retractions. We also recommend that the discipline of psychology gives a greater priority to the publication of replication studies; initiates a debate about how to respond to failed replications; adopts a more critical attitude to the importance of attaining statistical significance; discourages p-hacking and Hypothesizing After Results are Known (HARKing); assesses the long-term effects of pre-registering research; and supports stronger procedures to attest to the authenticity of data in research papers. Our contribution locates these issues in the context of a growing crisis of confidence in the value of social science research. We also challenge individual researchers to reassert the primacy of disinterested academic inquiry above pressures that can lead to an erosion of scholarly integrity.},
  keywords = {Misconduct,Psychology,Replication,Research,Retractions},
  file = {C\:\\Users\\Sarah von Grebmer\\Zotero\\storage\\B29FXK85\\Craig et al. - 2020 - Using retracted journal articles in psychology to understand research misconduct in the social scien.pdf;C\:\\Users\\Sarah von Grebmer\\Zotero\\storage\\8TU9FL26\\S004873332030010X.html}
}

@misc{cruwellWhatsBadgeComputational2022,
  title = {What's in a Badge? {{A}} Computational Reproducibility Investigation of the {{Open Data}} Badge Policy in One {{Issue}} of {{Psychological Science}}},
  shorttitle = {What's in a {{Badge}}?},
  author = {Cr{\"u}well, Sophia and Apthorp, Deborah and Baker, Bradley James and Colling, Lincoln J and Elson, Malte and Geiger, Sandra Jeanette and Lobentanzer, Sebastian and Mon{\'e}ger, Jean and Patterson, Alex and Schwarzkopf, D. Samuel and Zaneva, Mirela and Brown, Nicholas John Laird},
  year = 2022,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/729qt},
  urldate = {2025-12-18},
  abstract = {[Preprint; Manuscript accepted at Psychological Science] In April 2019, Psychological Science published its first issue in which all research articles received the Open Data badge. We used that issue to investigate the effectiveness of this badge, focusing on the adherence to its aim at Psychological Science: sharing both data and code to ensure reproducibility of results. Twelve researchers of varying experience levels attempted to reproduce the results of the empirical articles in the target issue (at least three researchers per article). We found that while all 14 articles provided at least some data and six provided analysis code, only one article was rated to be exactly reproducible, and three essentially reproducible with minor deviations. We suggest that researchers should be encouraged to adhere to the higher standard in force at Psychological Science. Moreover, a check of reproducibility during peer review may be preferable to the `disclosure method' of awarding badges.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\YBZYRZXM\Crüwell et al. - 2022 - What’s in a Badge A Computational Reproducibility Investigation of the Open Data Badge Policy in on.pdf}
}

@article{devriesCumulativeEffectReporting2018,
  title = {The Cumulative Effect of Reporting and Citation Biases on the Apparent Efficacy of Treatments: The Case of Depression},
  author = {{de Vries}, Y. A. and Roest, A. M. and {de Jonge}, P. and Cuijpers, P. and Munaf{\`o}, Marcus R. and Bastiaansen, J. A.},
  year = 2018,
  journal = {Psychological Medicine},
  volume = {48},
  number = {15},
  pages = {2453--2455},
  doi = {10.1017/S0033291718001873},
  urldate = {2025-12-09},
  file = {C\:\\Users\\Sarah von Grebmer\\Zotero\\storage\\UYZKQYDL\\The cumulative effect of reporting and citation biases on the apparent efficacy of treatments the c.pdf;C\:\\Users\\Sarah von Grebmer\\Zotero\\storage\\9D2AVGGN\\71D73CADE32C0D3D996DABEA3FCDBF57.html}
}

@article{donthuHowConductBibliometric2021,
  title = {How to Conduct a Bibliometric Analysis: {{An}} Overview and Guidelines},
  shorttitle = {How to Conduct a Bibliometric Analysis},
  author = {Donthu, Naveen and Kumar, Satish and Mukherjee, Debmalya and Pandey, Nitesh and Lim, Weng Marc},
  year = 2021,
  month = sep,
  journal = {Journal of Business Research},
  volume = {133},
  pages = {285--296},
  issn = {01482963},
  doi = {10.1016/j.jbusres.2021.04.070},
  urldate = {2025-09-25},
  langid = {english}
}

@article{dworkinExtentDriversGender2020,
  title = {The Extent and Drivers of Gender Imbalance in Neuroscience Reference Lists},
  author = {Dworkin, Jordan D. and Linn, Kristin A. and Teich, Erin G. and Zurn, Perry and Shinohara, Russell T. and Bassett, Danielle S.},
  year = 2020,
  month = aug,
  journal = {Nature Neuroscience},
  volume = {23},
  number = {8},
  pages = {918--926},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-020-0658-y},
  urldate = {2025-09-25},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\E8K4YMII\Dworkin et al. - 2020 - The extent and drivers of gender imbalance in neuroscience reference lists.pdf}
}

@article{emsleyChatGPTTheseAre2023,
  title = {{{ChatGPT}}: These Are Not Hallucinations -- They're Fabrications and Falsifications},
  shorttitle = {{{ChatGPT}}},
  author = {Emsley, Robin},
  year = 2023,
  journal = {Schizophrenia},
  volume = {9},
  number = {1},
  pages = {52},
  publisher = {Nature Publishing Group},
  issn = {2754-6993},
  doi = {10.1038/s41537-023-00379-4},
  urldate = {2025-12-10},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Cognitive Psychology,general,Medicine/Public Health,Neurology,Neurosciences,Psychiatry},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\SXVY4M54\Emsley - 2023 - ChatGPT these are not hallucinations – they’re fabrications and falsifications.pdf}
}

@article{eubankLessonsDecadeReplications2016,
  title = {Lessons from a {{Decade}} of {{Replications}} at the {{Quarterly Journal}} of {{Political Science}}},
  author = {Eubank, Nicholas},
  year = 2016,
  journal = {PS: Political Science \& Politics},
  volume = {49},
  number = {2},
  pages = {273--276},
  issn = {1049-0965, 1537-5935},
  doi = {10.1017/S1049096516000196},
  urldate = {2025-11-12},
  abstract = {To allow researchers to investigate not only whether a paper's methods are theoretically sound but also whether they have been properly implemented and are robust to alternative specifications, it is necessary that published papers be accompanied by their underlying data and code. This article describes experiences and lessons learned at the Quarterly Journal of Political Science since it began requiring authors to provide this type of replication code in 2005. It finds that of the 24 empirical papers subjected to in-house replication review since September 2012, only four packages did not require any modifications. Most troubling, 14 packages (58\%) had results in the paper that differed from those generated by the author's own code. Based on these experiences, this article presents a set of guidelines for authors and journals for improving the reliability and usability of replication packages.},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\BL5YQSWY\Eubank - 2016 - Lessons from a Decade of Replications at the Quarterly Journal of Political Science.pdf}
}

@article{fanelliPositiveResultsIncrease2010,
  title = {``{{Positive}}'' Results Increase down the Hierarchy of the Sciences},
  author = {Fanelli, Daniele},
  year = 2010,
  journal = {PLOS ONE},
  volume = {5},
  number = {4},
  pages = {e10068},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0010068},
  urldate = {2025-10-30},
  abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the ``hardness'' of scientific research---i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors---is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a ``positive'' (full or partial) or ``negative'' support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in ``softer'' sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
  langid = {english},
  keywords = {Forecasting,Mental health and psychiatry,Physical sciences,Scientists,Social psychology,Social research,Social sciences,Sociology},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\V63DT94Q\Fanelli - 2010 - “Positive” Results Increase Down the Hierarchy of the Sciences.pdf}
}

@article{franckScientificCommunicationAVanity1999,
  title = {Scientific {{Communication--A Vanity Fair}}?},
  author = {Franck, Georg},
  year = 1999,
  month = oct,
  journal = {Science},
  volume = {286},
  number = {5437},
  pages = {53--55},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.286.5437.53},
  urldate = {2025-09-25},
  langid = {english}
}

@misc{fulvioGenderImbalanceCitation2020,
  title = {Gender (Im)Balance in Citation Practices in Cognitive Neuroscience},
  author = {Fulvio, Jacqueline M. and Akinnola, Ileri and Postle, Bradley R.},
  year = 2020,
  month = aug,
  publisher = {{Scientific Communication and Education}},
  doi = {10.1101/2020.08.19.257402},
  urldate = {2025-09-25},
  abstract = {Abstract                        In the field of neuroscience, despite the fact that the proportion of peer-reviewed publications authored by women has increased in recent decades, the proportion of citations of women-led publications has not seen a commensurate increase: In five broad-scope journals, citations of papers first- and/or last-authored by women have been shown to be fewer than would be expected if gender was not a factor in citation decisions (Dworkin et al., 2020). Given the important implications that such underrepresentation may have on the careers of women researchers, it is important to determine whether this same trend is true in subdisciplines of the field, where interventions might be more effective. Here, we report the results of an extension of the analyses carried out by Dworkin et al. (2020) to citation patterns in the Journal of Cognitive Neuroscience (             JoCN             ). The results indicate that the underrepresentation of women-led publications in reference sections is also characteristic of papers published in             JoCN             over the past decade. Furthermore, this pattern of citation imbalances is present for all gender classes of authors, implicating systemic factors. These results contribute to the growing body of evidence that intentional action is needed to address inequities in the way that we carry out and communicate our science.},
  archiveprefix = {Scientific Communication and Education},
  copyright = {http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\XDPKJ59D\Fulvio et al. - 2020 - Gender (im)balance in citation practices in cognitive neuroscience.pdf}
}

@article{gaskinsVisibleNameChanges2021,
  title = {Visible Name Changes Promote Inequity for Transgender Researchers},
  author = {Gaskins, Leo Chan and McClain, Craig R.},
  year = 2021,
  month = mar,
  journal = {PLOS Biology},
  volume = {19},
  number = {3},
  pages = {e3001104},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3001104},
  urldate = {2025-09-25},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\NZDF3XVH\Gaskins and McClain - 2021 - Visible name changes promote inequity for transgender researchers.pdf}
}

@article{ghosalFindingResearchLineage2021,
  title = {Towards {{Finding}} a {{Research Lineage Leveraging}} on {{Identification}} of {{Significant Citations}}},
  author = {Ghosal, Tirthankar and Singh, Muskaan},
  year = 2021,
  month = oct,
  journal = {Proceedings of the Association for Information Science and Technology},
  volume = {58},
  number = {1},
  pages = {456--460},
  issn = {2373-9231, 2373-9231},
  doi = {10.1002/pra2.478},
  urldate = {2025-09-25},
  abstract = {Abstract             Finding the lineage of a research topic is crucial for understanding the prior state of the art and advancing scientific displacement. The deluge of scholarly articles makes it difficult to locate the most relevant prior work and causes researchers to spend a considerable amount of time building up their literature list. Citations play a significant role in discovering relevant literature. However, not all citations are created equal. A majority of the citations that a paper receives are for providing contextual, and background information to the citing papers and are not central to the theme of those papers. However, some papers are pivotal to the citing paper and inspire or stem up the research in the citing paper. Hence the nature of citation the former receives from the latter is significant. In this work in progress paper, we discuss our preliminary idea towards establishing a lineage for a given research via identifying significant citations. We hypothesize that such an automated system can facilitate relevant literature discovery and help identify knowledge flow for at least a certain category of papers. The distal goal of this work is to identify the real impact of research work or a facility beyond direct citation counts.},
  langid = {english}
}

@article{gopalakrishnaPrevalenceQuestionableResearch2022,
  title = {Prevalence of Questionable Research Practices, Research Misconduct and Their Potential Explanatory Factors: {{A}} Survey among Academic Researchers in {{The Netherlands}}},
  shorttitle = {Prevalence of Questionable Research Practices, Research Misconduct and Their Potential Explanatory Factors},
  author = {Gopalakrishna, Gowri and ter Riet, Gerben and Vink, Gerko and Stoop, Ineke and Wicherts, Jelte M. and Bouter, Lex M.},
  year = 2022,
  journal = {PLOS ONE},
  volume = {17},
  number = {2},
  pages = {e0263023},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0263023},
  urldate = {2025-11-12},
  abstract = {Prevalence of research misconduct, questionable research practices (QRPs) and their associations with a range of explanatory factors has not been studied sufficiently among academic researchers. The National Survey on Research Integrity targeted all disciplinary fields and academic ranks in the Netherlands. It included questions about engagement in fabrication, falsification and 11 QRPs over the previous three years, and 12 explanatory factor scales. We ensured strict identity protection and used the randomized response method for questions on research misconduct. 6,813 respondents completed the survey. Prevalence of fabrication was 4.3\% (95\% CI: 2.9, 5.7) and of falsification 4.2\% (95\% CI: 2.8, 5.6). Prevalence of QRPs ranged from 0.6\% (95\% CI: 0.5, 0.9) to 17.5\% (95\% CI: 16.4, 18.7) with 51.3\% (95\% CI: 50.1, 52.5) of respondents engaging frequently in at least one QRP. Being a PhD candidate or junior researcher increased the odds of frequently engaging in at least one QRP, as did being male. Scientific norm subscription (odds ratio (OR) 0.79; 95\% CI: 0.63, 1.00) and perceived likelihood of detection by reviewers (OR 0.62, 95\% CI: 0.44, 0.88) were associated with engaging in less research misconduct. Publication pressure was associated with more often engaging in one or more QRPs frequently (OR 1.22, 95\% CI: 1.14, 1.30). We found higher prevalence of misconduct than earlier surveys. Our results suggest that greater emphasis on scientific norm subscription, strengthening reviewers in their role as gatekeepers of research quality and curbing the ``publish or perish'' incentive system promotes research integrity.},
  langid = {english},
  keywords = {Deception,Linear regression analysis,Medical humanities,Medicine and health sciences,Open science,Research integrity,Scientific misconduct,Surveys},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\DPL2YJGJ\Gopalakrishna et al. - 2022 - Prevalence of questionable research practices, research misconduct and their potential explanatory f.pdf}
}

@article{guilleminDoingTrustHow2016,
  title = {``{{Doing Trust}}'': {{How Researchers Conceptualize}} and {{Enact Trust}} in {{Their Research Practice}}},
  shorttitle = {``{{Doing Trust}}''},
  author = {Guillemin, Marilys and Gillam, Lynn and Barnard, Emma and Stewart, Paul and Walker, Hannah and Rosenthal, Doreen},
  year = 2016,
  journal = {Journal of Empirical Research on Human Research Ethics},
  volume = {11},
  number = {4},
  pages = {370--381},
  publisher = {SAGE Publications Inc},
  issn = {1556-2646},
  doi = {10.1177/1556264616668975},
  urldate = {2025-11-20},
  abstract = {Trust in research is important but not well understood. We examine the ways that researchers understand and practice trust in research. Using a qualitative research design, we interviewed 19 researchers, including eight researchers involved in Australian Indigenous research. The project design focused on sensitive research including research involving vulnerable participants and sensitive research topics. Thematic analysis was used to analyze the data. We found that researchers' understanding of trust integrates both the conceptual and concrete; researchers understand trust in terms of how it relates to other similar concepts and how they practice trust in research. This provides a sound basis to better understand trust in research, as well as identifying mechanisms to regain trust when it is lost in research.},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\D2N77N4M\Guillemin et al. - 2016 - “Doing Trust” How Researchers Conceptualize and Enact Trust in Their Research Practice.pdf}
}

@article{guptaCitationMetricsEvaluation2025,
  title = {Citation Metrics and Evaluation of Journals and Conferences},
  author = {Gupta, Shikha and Kumar, Naveen and Bhalla, Subhash},
  year = 2025,
  month = jun,
  journal = {Journal of Information Science},
  volume = {51},
  number = {3},
  pages = {570--588},
  issn = {0165-5515, 1741-6485},
  doi = {10.1177/01655515231151411},
  urldate = {2025-09-25},
  abstract = {Citation analysis aims at evaluating the published scientific manuscripts, their authors and the publication venues (journals/conferences). There are several popular metrics for measuring the impact of the journals, the Impact Factor (IF) being the most popular. Similarly, the                                                                        H                                                                  -index is a popular metric for evaluating and ranking conferences. We have presented a review of metrics for citation analysis, categorised according to their applicability for evaluating journals and conferences. The citation metrics may also be categorised as popularity measuring and prestige measuring. Prestige measuring indicators like SCImago Journal Rank (SJR) and Eigenfactor have already gained popularity for evaluating journals. We discuss their role in evaluating the conferences. Indeed, some conferences have already started mentioning their prestige score in terms of the SJR of their conference proceedings. We also propose a Normalised Immediacy Index (                                                                        I                                                                     I                                                                       norm                                                                                                              ), a variant of the Immediacy Index (                                                                        II                                                                  ), to measure the immediate relevance of articles published in a journal/conference. It is shown that the proposed metric can be used for immediacy relevance comparison irrespective of the publication schedule of the articles. Spearman correlation was run to determine the relationship between the values of the proposed                                                                        I                                                                     I                                                                       norm                                                                                                              and traditional metrics (                                                                        H                                                                  -index for conferences and IF for journals). A strong, positive monotonic correlation was observed between                                                                        I                                                                     I                                                                       norm                                                                                                              and               H               -index (                                                                                                                        r                                                                       s                                                                                                              \,=\,.67,                                                                        n                                                                  \,=\,17,                                                                        p                                                                  \,{$<$}\,.01) for conferences and between                                                                        I                                                                     I                                                                       norm                                                                                                              and IF (                                                                                                                        r                                                                       s                                                                                                              \,=\,.65,                                                                        n                                                                  \,=\,20,                                                                        p                                                                  \,{$<$}\,.01) for journals.},
  langid = {english}
}

@article{gyaRegisteredReportsNew2023,
  title = {Registered {{Reports}}: {{A}} New Chapter at {{{\emph{Ecology}}}}{\emph{ \& }}{{{\emph{Evolution}}}}},
  shorttitle = {Registered {{Reports}}},
  author = {Gya, Ragnhild and Birkeli, Kristine and Dahle, Ingrid J. and Foote, Christopher G. and Geange, Sonya R. and Lynn, Joshua S. and T{\"o}pper, Joachim P. and Vandvik, Vigdis and Zernichow, Camilla and Jenkins, Gareth B.},
  year = 2023,
  month = apr,
  journal = {Ecology and Evolution},
  volume = {13},
  number = {4},
  pages = {e10023},
  issn = {2045-7758, 2045-7758},
  doi = {10.1002/ece3.10023},
  urldate = {2025-11-06},
  abstract = {Ecology \& Evolution has published its first Registered Report and offers the perspective of the editor, author, and student on the publication process.},
  langid = {english}
}

@article{hardwickeReducingBiasIncreasing2023,
  title = {Reducing Bias, Increasing Transparency and Calibrating Confidence with Preregistration},
  author = {Hardwicke, Tom E. and Wagenmakers, Eric-Jan},
  year = 2023,
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {7},
  number = {1},
  pages = {15--26},
  issn = {2397-3374},
  doi = {10.1038/s41562-022-01497-2},
  urldate = {2025-11-06},
  langid = {english}
}

@article{heinzRandomizedTrialGenerative2025,
  title = {Randomized Trial of a Generative {{AI}} Chatbot for Mental Health Treatment},
  author = {Heinz, Michael V. and Mackin, Daniel M. and Trudeau, Brianna M. and Bhattacharya, Sukanya and Wang, Yinzhou and Banta, Haley A. and Jewett, Abi D. and Salzhauer, Abigail J. and Griffin, Tess Z. and Jacobson, Nicholas C.},
  year = 2025,
  journal = {NEJM AI},
  volume = {2},
  number = {4},
  pages = {AIoa2400802},
  publisher = {Massachusetts Medical Society},
  doi = {10.1056/AIoa2400802},
  urldate = {2025-12-09}
}

@article{hendriksReplicationCrisisTrust2020,
  title = {Replication Crisis = Trust Crisis? {{The}} Effect of Successful vs Failed Replications on Laypeople's Trust in Researchers and Research},
  shorttitle = {Replication Crisis = Trust Crisis?},
  author = {Hendriks, Friederike and Kienhues, Dorothe and Bromme, Rainer},
  year = 2020,
  journal = {Public Understanding of Science},
  volume = {29},
  number = {3},
  pages = {270--288},
  publisher = {SAGE Publications Ltd},
  issn = {0963-6625},
  doi = {10.1177/0963662520902383},
  urldate = {2025-11-19},
  abstract = {In methodological and practical debates about replications in science, it is (often implicitly) assumed that replications will affect public trust in science. In this preregistered experiment (N\,=\,484), we varied (a) whether a replication attempt was successful or not and (b) whether the replication was authored by the same, or another lab. Results showed that ratings of study credibility (e.g. evidence strength, {$\eta$}P2\,=\,.15) and researcher trustworthiness (e.g. expertise, {$\eta$}P2\,=\,.15) were rated higher upon learning of replication success, and lower in case of replication failure. The replication's author did not make a meaningful difference. Prior beliefs acted as covariate for ratings of credibility, but not trustworthiness, while epistemic beliefs regarding the certainty of knowledge were a covariate to both. Hence, laypeople seem to notice that successfully replicated results entail higher epistemic significance, while possibly not taking into account that replications should be conducted by other labs.},
  langid = {english}
}

@article{hirschIndexQuantifyIndividuals2005,
  title = {An Index to Quantify an Individual's Scientific Research Output},
  author = {Hirsch, J. E.},
  year = 2005,
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {102},
  number = {46},
  pages = {16569--16572},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0507655102},
  urldate = {2025-09-25},
  abstract = {I propose the index               h               , defined as the number of papers with citation number {$\geq$}               h               , as a useful index to characterize the scientific output of a researcher.},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\9K2W4BA8\Hirsch - 2005 - An index to quantify an individual's scientific research output.pdf}
}

@article{hoffmannMultiplicityAnalysisStrategies2021,
  title = {The Multiplicity of Analysis Strategies Jeopardizes Replicability: Lessons Learned across Disciplines},
  shorttitle = {The Multiplicity of Analysis Strategies Jeopardizes Replicability},
  author = {Hoffmann, Sabine and Sch{\"o}nbrodt, Felix and Elsas, Ralf and Wilson, Rory and Strasser, Ulrich and Boulesteix, Anne-Laure},
  year = 2021,
  journal = {Royal Society Open Science},
  volume = {8},
  number = {4},
  pages = {rsos.201925, 201925},
  issn = {2054-5703},
  doi = {10.1098/rsos.201925},
  urldate = {2025-10-30},
  abstract = {For a given research question, there are usually a large variety of possible analysis strategies acceptable according to the scientific standards of the field, and there are concerns that this multiplicity of analysis strategies plays an important role in the non-replicability of research findings. Here, we define a general framework on common sources of uncertainty arising in computational analyses that lead to this multiplicity, and apply this framework within an overview of approaches proposed across disciplines to address the issue. Armed with this framework, and a set of recommendations derived therefrom, researchers will be able to recognize strategies applicable to their field and use them to generate findings more likely to be replicated in future studies, ultimately improving the credibility of the scientific process.},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\8JY3KW63\Hoffmann et al. - 2021 - The multiplicity of analysis strategies jeopardizes replicability lessons learned across discipline.pdf}
}

@article{hoffmanTenSimpleRules2024,
  title = {Ten Simple Rules for Teaching an Introduction to {{R}}},
  author = {Hoffman, Ava M. and Wright, Carrie},
  year = 2024,
  journal = {PLOS Computational Biology},
  volume = {20},
  number = {5},
  pages = {e1012018},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1012018},
  urldate = {2026-01-23},
  langid = {english},
  keywords = {Computer software,Data visualization,Ecosystems,Human learning,Instructors,Language,Language acquisition,Learning curves},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\SL336PFX\Hoffman and Wright - 2024 - Ten simple rules for teaching an introduction to R.pdf}
}

@misc{ihlePreregistrationRegisteredReport2023,
  title = {({{Pre}})Registration and Registered Report {{Q}}\&{{A}}},
  author = {Ihle, Malika},
  year = 2023
}

@article{ioannidisWhatMetaresearchHas2025,
  title = {What Meta-Research Has Taught Us about Research and Changes to Research Practices},
  author = {Ioannidis, John P. A.},
  year = 2025,
  journal = {Journal of Economic Surveys},
  volume = {39},
  number = {4},
  pages = {1823--1834},
  issn = {1467-6419},
  doi = {10.1111/joes.12666},
  urldate = {2025-11-19},
  abstract = {Meta-research has become increasingly popular and has provided interesting insights on what can go well and what can go wrong with research practices and scientific studies. Many stakeholders are taking actions to try to solve problems and biases identified through meta-research. However, very often there is little or no evidence that specific recommendations and actions may actually lead to improvements and a favorable benefit-harm ratio. The current commentary offers an eclectic overview of what we have learned from meta-research efforts (mostly observational, but also some quasi-experimental and experimental work) and what the implications of this evidence may be for changing research practices. Areas discussed include the study (and differentiation) of genuine effects and biases, fraud (including the impact of new technologies), peer review, replication and reproducibility checks, transparency indicators, and the interface of research practices with reward systems. Meta-research has offered on all of these fronts empirical evidence that sometimes pertains even to large effects of extreme biases. Continued surveys of research practices and results may offer timely updates of the status of research and its biases, as these may change markedly over time. Meta-research should be seen as part of research, not separate from it, in their concurrent evolution.},
  copyright = {\copyright{} 2024 John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {bias,incentives,meta-research,replication,research practices,transparency},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\UYRB9SXU\joes.html}
}

@incollection{jacksonDailyGrind1990,
  title = {The {{Daily Grind}}},
  booktitle = {Life in {{Classrooms}}},
  author = {Jackson, Philip},
  year = 1990,
  edition = {2nd},
  pages = {34--35},
  publisher = {Teachers College Press},
  isbn = {978-0-8077-3034-8},
  langid = {english}
}

@article{johnMeasuringPrevalenceQuestionable2012,
  title = {Measuring the Prevalence of Questionable Research Practices with Incentives for Truth Telling},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = 2012,
  journal = {Psychological Science},
  volume = {23},
  number = {5},
  pages = {524--532},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797611430953},
  urldate = {2025-10-30},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  langid = {english}
}

@article{kaplanLikelihoodNullEffects2015,
  title = {Likelihood of {{Null Effects}} of {{Large NHLBI Clinical Trials Has Increased}} over {{Time}}},
  author = {Kaplan, Robert M. and Irvin, Veronica L.},
  editor = {Garattini, Silvio},
  year = 2015,
  month = aug,
  journal = {PLOS ONE},
  volume = {10},
  number = {8},
  pages = {e0132382},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0132382},
  urldate = {2025-11-06},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\TUPL2AA7\Kaplan and Irvin - 2015 - Likelihood of Null Effects of Large NHLBI Clinical Trials Has Increased over Time.pdf}
}

@article{kerrHARKingHypothesizingResults1998,
  title = {{{HARKing}}: {{Hypothesizing After}} the {{Results}} Are {{Known}}},
  shorttitle = {{{HARKing}}},
  author = {Kerr, Norbert L.},
  year = 1998,
  month = aug,
  journal = {Personality and Social Psychology Review},
  volume = {2},
  number = {3},
  pages = {196--217},
  issn = {1088-8683, 1532-7957},
  doi = {10.1207/s15327957pspr0203_4},
  urldate = {2025-11-06},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
  langid = {english}
}

@article{kerrHARKingHypothesizingResults1998a,
  title = {{{HARKing}}: {{Hypothesizing After}} the {{Results}} Are {{Known}}},
  shorttitle = {{{HARKing}}},
  author = {Kerr, Norbert L.},
  year = 1998,
  journal = {Personality and Social Psychology Review},
  volume = {2},
  number = {3},
  pages = {196--217},
  publisher = {SAGE Publications Inc},
  issn = {1088-8683},
  doi = {10.1207/s15327957pspr0203_4},
  urldate = {2025-11-12},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  langid = {english}
}

@article{kojakuDetectingAnomalousCitation2021,
  title = {Detecting Anomalous Citation Groups in Journal Networks},
  author = {Kojaku, Sadamori and Livan, Giacomo and Masuda, Naoki},
  year = 2021,
  month = jul,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {14524},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-93572-3},
  urldate = {2025-09-25},
  abstract = {Abstract             The ever-increasing competitiveness in the academic publishing market incentivizes journal editors to pursue higher impact factors. This translates into journals becoming more selective, and, ultimately, into higher publication standards. However, the fixation on higher impact factors leads some journals to artificially boost impact factors through the coordinated effort of a ``citation cartel'' of journals. ``Citation cartel'' behavior has become increasingly common in recent years, with several instances being reported. Here, we propose an algorithm---named CIDRE---to detect anomalous groups of journals that exchange citations at excessively high rates when compared against a null model that accounts for scientific communities and journal size. CIDRE detects more than half of the journals suspended from Journal Citation Reports due to anomalous citation behavior in the year of suspension or in advance. Furthermore, CIDRE detects many new anomalous groups, where the impact factors of the member journals are lifted substantially higher by the citations from other member journals. We describe a number of such examples in detail and discuss the implications of our findings with regard to the current academic climate.},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\WB5PPUMV\Kojaku et al. - 2021 - Detecting anomalous citation groups in journal networks.pdf}
}

@article{laceyRoadmapGoodPractice2025,
  title = {A Roadmap to Good Practice for Training Supervisors and Leadership: A {{European}} Perspective},
  shorttitle = {A Roadmap to Good Practice for Training Supervisors and Leadership},
  author = {Lacey, Se{\'a}n and Haven, Tamarinde and Santos, Rita and Farrelly, Tom and Murray, M{\'a}ir{\'e}ad and Kavouras, Panagiotis},
  year = 2025,
  journal = {Frontiers in Research Metrics and Analytics},
  volume = {10},
  publisher = {Frontiers},
  issn = {2504-0537},
  doi = {10.3389/frma.2025.1531467},
  urldate = {2026-01-23},
  abstract = {PurposeSupervision and leadership are regarded to have a major role in promoting responsible research. Various approaches to training for supervisors and leaders have been proposed. However, little is known about what works best, what kind of hurdles are faced in implementation and engagement, and what methods of assessing the effectiveness of training programs are available. Through exploring these points, this research aims to propose a roadmap to good practice for training supervisors and leadership.DesignA virtual marketplace for exchanging current practices and approaches for training supervisors and leadership took place in March 2024. Twenty-two policy makers from thirteen European countries, supervisors and senior research leaders were selected to participate, using opportunistic and purposive sampling. Facilitated using the Gather platform, the marketplace commenced with a non-European keynote speaker on training supervisors and leadership. Three main questions were brought forward for discussion separately---What works well for successful implementation? What are the challenges? How do we assess effectiveness? After the keynote presentation, marketplace participants rotated in groups between three market stalls to share thoughts on good practices for training supervisors and leadership framed around the three questions. Moderators for each of the stalls recorded detailed field notes to inform the study findings.FindingsDuring the exchange, mandatory training, especially when tailored to specific disciplines and conducted in small groups using a problem-based learning approach, was deemed effective. Awareness of power imbalances between early career researchers, supervisors, and leaders were to the fore. Critical challenges included a need for senior supervisors and leaders to participate and support research training. Also a need for systemic processes, tailored to specific local settings to avoid ad hoc implementation of policies, procedures and training. In assessing effectiveness there was an emphasis to share more research data and to utilize incidents of breaches of research integrity. The latter to be leveraged for learning purposes and transparency around the investigative process.OriginalityThere are multiple facets to good practice for training supervisors and leadership, along with a multitude of practices, however there is little evidence of practices that work, challenges around implementation, and assessing effectiveness.},
  langid = {english},
  keywords = {leadership,open science,research culture,research integrity,training supervisors},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\95FGE9UF\Lacey et al. - 2025 - A roadmap to good practice for training supervisors and leadership a European perspective.pdf}
}

@article{lautrupHearttoheartChatGPTImpact2023,
  title = {Heart-to-Heart with {{ChatGPT}}: The Impact of Patients Consulting {{AI}} for Cardiovascular Health Advice},
  shorttitle = {Heart-to-Heart with {{ChatGPT}}},
  author = {Lautrup, Anton Danholt and Hyrup, Tobias and {Schneider-Kamp}, Anna and Dahl, Marie and Lindholt, Jes Sanddal and {Schneider-Kamp}, Peter},
  year = 2023,
  journal = {Open Heart},
  volume = {10},
  number = {2},
  publisher = {British Cardiovascular Society},
  issn = {2053-3624},
  doi = {10.1136/openhrt-2023-002455},
  urldate = {2025-12-09},
  abstract = {Objectives The advent of conversational artificial intelligence (AI) systems employing large language models such as ChatGPT has sparked public, professional and academic debates on the capabilities of such technologies. This mixed-methods study sets out to review and systematically explore the capabilities of ChatGPT to adequately provide health advice to patients when prompted regarding four topics from the field of cardiovascular diseases.Methods As of 30 May 2023, 528 items on PubMed contained the term ChatGPT in their title and/or abstract, with 258 being classified as journal articles and included in our thematic state-of-the-art review. For the experimental part, we systematically developed and assessed 123 prompts across the four topics based on three classes of users and two languages. Medical and communications experts scored ChatGPT's responses according to the 4Cs of language model evaluation proposed in this article: correct, concise, comprehensive and comprehensible.Results The articles reviewed were fairly evenly distributed across discussing how ChatGPT could be used for medical publishing, in clinical practice and for education of medical personnel and/or patients. Quantitatively and qualitatively assessing the capability of ChatGPT on the 123 prompts demonstrated that, while the responses generally received above-average scores, they occupy a spectrum from the concise and correct via the absurd to what only can be described as hazardously incorrect and incomplete. Prompts formulated at higher levels of health literacy generally yielded higher-quality answers. Counterintuitively, responses in a lower-resource language were often of higher quality.Conclusions The results emphasise the relationship between prompt and response quality and hint at potentially concerning futures in personalised medicine. The widespread use of large language models for health advice might amplify existing health inequalities and will increase the pressure on healthcare systems by providing easy access to many seemingly likely differential diagnoses and recommendations for seeing a doctor for even harmless ailments.},
  copyright = {This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license},
  langid = {english},
  pmid = {10.1136/openhrt-2023-002455},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\2WW4A66C\Lautrup et al. - 2023 - Heart-to-heart with ChatGPT the impact of patients consulting AI for cardiovascular health advice.pdf}
}

@article{levittMultidisciplinaryResearchMore2008,
  title = {Is Multidisciplinary Research More Highly Cited? {{A}} Macrolevel Study},
  shorttitle = {Is Multidisciplinary Research More Highly Cited?},
  author = {Levitt, Jonathan M. and Thelwall, Mike},
  year = 2008,
  month = oct,
  journal = {Journal of the American Society for Information Science and Technology},
  volume = {59},
  number = {12},
  pages = {1973--1984},
  issn = {1532-2882, 1532-2890},
  doi = {10.1002/asi.20914},
  urldate = {2025-09-25},
  abstract = {Abstract                            Interdisciplinary collaboration is a major goal in research policy. This study uses citation analysis to examine diverse subjects in the               Web of Science               and Scopus to ascertain whether, in general, research published in journals classified in more than one subject is more highly cited than research published in journals classified in a single subject. For each subject, the study divides the journals into two disjoint sets called               Multi               and               Mono               . Multi consists of all journals in the subject and at least one other subject whereas Mono consists of all journals in the subject and in no other subject. The main findings are: (a) For social science subject categories in both the               Web of Science               and Scopus, the average citation levels of articles in Mono and Multi are very similar; and (b) for Scopus subject categories within life sciences, health sciences, and physical sciences, the average citation level of Mono articles is roughly twice that of Multi articles. Hence, one cannot assume that in general, multidisciplinary research will be more highly cited, and the converse is probably true for many areas of science. A policy implication is that, at least in the sciences, multidisciplinary researchers should not be evaluated by citations on the same basis as monodisciplinary researchers.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english}
}

@article{linBibliometricVisualAnalysis2022,
  title = {Bibliometric and Visual Analysis of Coronary Microvascular Dysfunction},
  author = {Lin, Xiaoxiao and Wu, Guomin and Gao, Beibei and Wang, Shuai and Huang, Jinyu},
  year = 2022,
  month = nov,
  journal = {Frontiers in Cardiovascular Medicine},
  volume = {9},
  pages = {1021346},
  issn = {2297-055X},
  doi = {10.3389/fcvm.2022.1021346},
  urldate = {2025-09-25},
  abstract = {Background               Coronary microvascular dysfunction (CMD) may play an important role in various cardiovascular diseases, including HFpEF and both obstructive and non-obstructive coronary artery disease (CAD). To date, there has been no bibliometric analysis to summarize this field. Here, we aim to conduct a bibliometric analysis of CMD to determine the current status and frontiers in this field.                                         Materials and methods               Publications about CMD were taken from the Web of Science Core Collection database (WOSCC). WOSCC's literature analysis wire, the VOSviewer 1.6.16, and CiteSpace 5.1.3 were used to conduct the analysis.                                         Results               A total of 785 publications containing 206 reviews and 579 articles are included in the sample. The leading authors are Iacopo Olivotto, Paolo G. Camici, and Carl J. Pepine. The most productive institutions are the University of Florence, Cedars Sinai Medical Center, and Harvard University. The most productive countries are the USA, Italy, and England. There are a total of 237 journals that contribute to this field, and the leading journals in our study were the International Journal of Cardiology, the European Heart Journal and the JACC. From 2012 to 2021, the top three most-cited articles focused on the association between HFpEF and CMD. The important keywords are heart failure, hypertrophic cardiomyopathy, chest pain, women, coronary flow reserve (CFR), endothelial dysfunction and prognostic value. ``Positron emission tomography'' shows the strongest burst strength, followed by ``blow flow'' and ``artery.'' The keywords that started to burst from 2015 are particularly emphasized, including ``heart failure,'' ``coronary flow reserve,'' and ``management.''                                         Conclusion               Studies about CMD are relatively limited, and the largest contribution comes from the USA, Italy and England. More studies are needed, and publications from other countries should be enhanced. The main research hotspots in the CMD field include CMD in patients with HFpEF, sex differences, the new methods of diagnosis for CMD, and the effective treatment of CMD. Attention should be given to CMD in patients with HFpEF, and untangling the association between CMD and HFpEF could be helpful in the development of physiology-stratified treatment for patients with CMD and HFpEF.},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\FJ4FYWNN\Lin et al. - 2022 - Bibliometric and visual analysis of coronary microvascular dysfunction.pdf}
}

@article{liuNonWhiteScientistsAppear2023,
  title = {Non-{{White}} Scientists Appear on Fewer Editorial Boards, Spend More Time under Review, and Receive Fewer Citations},
  author = {Liu, Fengyuan and Rahwan, Talal and AlShebli, Bedoor},
  year = 2023,
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {13},
  pages = {e2215324120},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2215324120},
  urldate = {2025-09-25},
  abstract = {Disparities continue to pose major challenges in various aspects of science. One such aspect is editorial board composition, which has been shown to exhibit racial and geographical disparities. However, the literature on this subject lacks longitudinal studies quantifying the degree to which the racial composition of editors reflects that of scientists. Other aspects that may exhibit racial disparities include the time spent between the submission and acceptance of a manuscript and the number of citations a paper receives relative to textually similar papers, but these have not been studied to date. To fill this gap, we compile a dataset of 1,000,000 papers published between 2001 and 2020 by six publishers, while identifying the handling editor of each paper. Using this dataset, we show that most countries in Asia, Africa, and South America (where the majority of the population is ethnically non-White) have fewer editors than would be expected based on their share of authorship. Focusing on US-based scientists reveals Black as the most underrepresented race. In terms of acceptance delay, we find, again, that papers from Asia, Africa, and South America spend more time compared to other papers published in the same journal and the same year. Regression analysis of US-based papers reveals that Black authors suffer from the greatest delay. Finally, by analyzing citation rates of US-based papers, we find that Black and Hispanic scientists receive significantly fewer citations compared to White ones doing similar research. Taken together, these findings highlight significant challenges facing non-White scientists.},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\36M9HW68\Liu et al. - 2023 - Non-White scientists appear on fewer editorial boards, spend more time under review, and receive few.pdf}
}

@article{mayo-wilsonConsistentPreciseDescription2025,
  title = {Consistent and {{Precise Description}} of {{Research Outputs Could Improve Implementation}} of {{Open Science}}},
  author = {{Mayo-Wilson}, Evan and Grant, Sean and Corker, Katherine S. and Moher, David},
  year = 2025,
  month = oct,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {8},
  number = {4},
  pages = {25152459251375445},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/25152459251375445},
  urldate = {2025-11-06},
  abstract = {In 2013, the Center for Open Science proposed that journal articles be awarded ``badges'' for engaging in open-science practices, including preregistration. In 2015, the Transparency and Openness Promotion (TOP) guidelines (TOP 2015) promoted preregistration of studies and analysis plans. Since then, the term ``preregistration'' has been used to describe different research outputs created at different times---sometimes, but not always, including study registration. Following a review of evidence about TOP 2015 implementation, including evidence that adherence could not be rated reliably, the TOP Guidelines Advisory Board updated these guidelines (TOP 2025). The TOP 2025 guidelines no longer use the term ``preregistration.'' Instead, TOP 2025 disambiguates specific research outputs, such as registrations, study protocols, analysis plans, code, and other research materials. TOP 2025 also explains that researchers should describe the time at which outputs are created and shared in relation to key study activities. In this article, we explain why adopting the terminology used in TOP 2025 and describing the times at which specific research outputs are created and shared will enhance understanding and support better implementation and reporting of open science.},
  langid = {english}
}

@article{mehoGenderGapHighly2022,
  title = {Gender Gap among Highly Cited Researchers, 2014--2021},
  author = {Meho, Lokman I.},
  year = 2022,
  month = dec,
  journal = {Quantitative Science Studies},
  volume = {3},
  number = {4},
  pages = {1003--1023},
  issn = {2641-3337},
  doi = {10.1162/qss_a_00218},
  urldate = {2025-09-25},
  abstract = {Abstract             This study examines the extent to which women are represented among the world's highly cited researchers (HCRs) and explores their representation over time and across fields, regions, and countries. The study identifies 11,842 HCRs in all fields and uses Gender-API, Genderize.Io, Namsor, and the web to identify their gender. Women's share of HCRs grew from 13.1\% in 2014 to 14.0\% in 2021; however, the increase is slower than that of women's representation among the general population of authors. The data show that women's share of HCRs would need to increase by 100\% in health and social sciences, 200\% in agriculture, biology, earth, and environmental sciences, 300\% in mathematics and physics, and 500\% in chemistry, computer science, and engineering to close the gap with men. Women's representation among all HCRs in North America, Europe, and Oceania ranges from 15\% to 18\%, compared to a world average of 13.7\%. Among countries with the highest number of HCRs, the gender gap is least evident in Switzerland, Brazil, Norway, the United Kingdom, and the United States and most noticeable in Asian countries. The study reviews factors that can be seen to influence the gender gap among HCRs and makes recommendations for improvement.},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\PIWGIJMS\Meho - 2022 - Gender gap among highly cited researchers, 2014–2021.pdf}
}

@article{mertonMatthewEffectScience1968,
  title = {The {{Matthew Effect}} in {{Science}}: {{The}} Reward and Communication Systems of Science Are Considered.},
  shorttitle = {The {{Matthew Effect}} in {{Science}}},
  author = {Merton, Robert K.},
  year = 1968,
  month = jan,
  journal = {Science},
  volume = {159},
  number = {3810},
  pages = {56--63},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.159.3810.56},
  urldate = {2025-09-25},
  abstract = {This account of the Matthew effect is another small exercise in the psychosociological analysis of the workings of science as a social institution. The initial problem is transformed by a shift in theoretical perspective. As originally identified, the Matthew effect was construed in terms of enhancement of the position of already eminent scientists who are given disproportionate credit in cases of collaboration or of independent multiple discoveries. Its significance was thus confined to its implications for the reward system of science. By shifting the angle of vision, we note other possible kinds of consequences, this time for the communication system of science. The Matthew effect may serve to heighten the visibility of contributions to science by scientists of acknowledged standing and to reduce the visibility of contributions by authors who are less well known. We examine the psychosocial conditions and mechanisms underlying this effect and find a correlation between the redundancy function of multiple discoveries and the focalizing function of eminent men of science---a function which is reinforced by the great value these men place upon finding basic problems and by their self-assurance. This self-assurance, which is partly inherent, partly the result of experiences and associations in creative scientific environments, and partly a result of later social validation of their position, encourages them to search out risky but important problems and to highlight the results of their inquiry. A macrosocial version of the Matthew principle is apparently involved in those processes of social selection that currently lead to the concentration of scientific resources and talent.},
  langid = {english}
}

@unpublished{middletonAcademicWheelPrivilege2025,
  title = {The {{Academic Wheel}} of {{Privilege}}: {{An}} Equity-Based Tool for Authorship Order},
  author = {Middleton, Sara L and Iley, Bethan and Sulik, Justin and Elsherif, Mahmoud and Azevedo, Flavio},
  year = 2025
}

@misc{movonoIndigenousScholarsStruggle2021,
  type = {The {{Conversation}}},
  title = {Indigenous Scholars Struggle to Be Heard in the Mainstream. {{Here}}'s How Journal Editors and Reviewers Can Help},
  author = {Movono, Apisalome and Carr, Anna and Hughes, Emma and {Higgins-Desbiolles}, Freya and Hapeta, Jeremy William and Scheyvens, Regina and {Stewart-Withers}, Rochelle},
  year = 2021
}

@article{mullerWorldingGeographyLinguistic2021,
  title = {Worlding Geography: {{From}} Linguistic Privilege to Decolonial Anywheres},
  shorttitle = {Worlding Geography},
  author = {M{\"u}ller, Martin},
  year = 2021,
  month = dec,
  journal = {Progress in Human Geography},
  volume = {45},
  number = {6},
  pages = {1440--1466},
  issn = {0309-1325, 1477-0288},
  doi = {10.1177/0309132520979356},
  urldate = {2025-09-25},
  abstract = {Geography studies the world. Our knowledge of the world, however, comes mostly from Anglophone sources. This makes Geography in urgent need of worlding~-- of including multiple voices and languages from around the world. Introducing the notion of linguistic privilege, the article establishes language as an important dimension of epistemic struggle, alongside gender, race, class and others. Its analysis finds the greatest linguistic privilege in the most influential positions in knowledge production~-- editors of handbooks and journals and authors of progress reports. Three strategies of worlding should challenge this: making gatekeepers multilingual, promoting multiple Englishes and valorising ex-centric knowledge.},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\VNK54Y4K\Müller - 2021 - Worlding geography From linguistic privilege to decolonial anywheres.pdf}
}

@article{munafoScientificRigorArt2014,
  title = {Scientific Rigor and the Art of Motorcycle Maintenance},
  author = {Munaf{\`o}, Marcus and Noble, Simon and Browne, William J. and Brunner, Dani and Button, Katherine and Ferreira, Joaquim and Holmans, Peter and Langbehn, Douglas and Lewis, Glyn and Lindquist, Martin and Tilling, Kate and Wagenmakers, Eric-Jan and Blumenstein, Robi},
  year = 2014,
  journal = {Nature Biotechnology},
  volume = {32},
  number = {9},
  pages = {871--873},
  publisher = {Nature Publishing Group},
  issn = {1546-1696},
  doi = {10.1038/nbt.3004},
  urldate = {2025-11-19},
  abstract = {The reliability of scientific research is under scrutiny. A recently convened working group proposes cultural adjustments to incentivize better research practices.},
  copyright = {2014 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Peer review,Publication characteristics,Research management},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\FZIM28DL\Munafò et al. - 2014 - Scientific rigor and the art of motorcycle maintenance.pdf}
}

@article{nosekPreregistrationRevolution2018,
  title = {The Preregistration Revolution},
  author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
  year = 2018,
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2600--2606},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708274114},
  urldate = {2025-11-06},
  abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes---a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\4VKVWHE9\Nosek et al. - 2018 - The preregistration revolution.pdf}
}

@article{oboyleChrysalisEffectHow2017,
  title = {The {{Chrysalis Effect}}: {{How}} Ugly Initial Results Metamorphosize into Beautiful Articles},
  shorttitle = {The {{Chrysalis Effect}}},
  author = {O'Boyle, Ernest Hugh and Banks, George Christopher and {Gonzalez-Mul{\'e}}, Erik},
  year = 2017,
  journal = {Journal of Management},
  volume = {43},
  number = {2},
  pages = {376--399},
  issn = {0149-2063, 1557-1211},
  doi = {10.1177/0149206314527133},
  urldate = {2025-10-30},
  abstract = {The issue of a published literature not representative of the population of research is most often discussed in terms of entire studies being suppressed. However, alternative sources of publication bias are questionable research practices (QRPs) that entail post hoc alterations of hypotheses to support data or post hoc alterations of data to support hypotheses. Using general strain theory as an explanatory framework, we outline the means, motives, and opportunities for researchers to better their chances of publication independent of rigor and relevance. We then assess the frequency of QRPs in management research by tracking differences between dissertations and their resulting journal publications. Our primary finding is that from dissertation to journal article, the ratio of supported to unsupported hypotheses more than doubled (0.82 to 1.00 versus 1.94 to 1.00). The rise in predictive accuracy resulted from the dropping of statistically nonsignificant hypotheses, the addition of statistically significant hypotheses, the reversing of predicted direction of hypotheses, and alterations to data. We conclude with recommendations to help mitigate the problem of an unrepresentative literature that we label the ``Chrysalis Effect.''},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\SZ8Q2RP5\O’Boyle et al. - 2017 - The Chrysalis Effect How Ugly Initial Results Metamorphosize Into Beautiful Articles.pdf}
}

@article{opensciencecollaborationEstimatingReproducibilityPsychological2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = 2015,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aac4716},
  urldate = {2025-12-09},
  abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\U9N23VW4\Open Science Collaboration - 2015 - Estimating the reproducibility of psychological science.pdf}
}

@article{parkOtherPeoplesWords2003,
  title = {In {{Other}} ({{People}}'s) {{Words}}: {{Plagiarism}} by University Students--Literature and Lessons},
  shorttitle = {In {{Other}} ({{People}}'s) {{Words}}},
  author = {Park, Chris},
  year = 2003,
  month = oct,
  journal = {Assessment \& Evaluation in Higher Education},
  volume = {28},
  number = {5},
  pages = {471--488},
  issn = {0260-2938, 1469-297X},
  doi = {10.1080/02602930301677},
  urldate = {2025-09-25},
  langid = {english}
}

@article{parsonsCommunitysourcedGlossaryOpen2022,
  title = {A Community-Sourced Glossary of Open Scholarship Terms},
  author = {Parsons, Sam and Azevedo, Fl{\'a}vio and Elsherif, Mahmoud M. and Guay, Samuel and Shahim, Owen N. and Govaart, Gisela H. and Norris, Emma and O'Mahony, Aoife and Parker, Adam J. and Todorovic, Ana and Pennington, Charlotte R. and {Garcia-Pelegrin}, Elias and Lazi{\'c}, Aleksandra and Robertson, Olly and Middleton, Sara L. and Valentini, Beatrice and McCuaig, Joanne and Baker, Bradley J. and Collins, Elizabeth and Fillon, Adrien A. and Lonsdorf, Tina B. and Lim, Michele C. and Vanek, Norbert and Kovacs, Marton and Roettger, Timo B. and Rishi, Sonia and Miranda, Jacob F. and Jaquiery, Matt and Stewart, Suzanne L. K. and Agostini, Valeria and Stewart, Andrew J. and Izydorczak, Kamil and {Ashcroft-Jones}, Sarah and Hartmann, Helena and Ingham, Madeleine and Yamada, Yuki and Vasilev, Martin R. and Dechterenko, Filip and {Albayrak-Aydemir}, Nihan and Yang, Yu-Fang and LaPlume, Annalise A. and Wolska, Julia K. and Henderson, Emma L. and Zaneva, Mirela and Farrar, Benjamin G. and Mounce, Ross and Kalandadze, Tamara and Li, Wanyin and Xiao, Qinyu and Ross, Robert M. and Yeung, Siu Kit and Liu, Meng and Vandegrift, Micah L. and Kekecs, Zoltan and Topor, Marta K. and Baum, Myriam A. and Williams, Emily A. and Assaneea, Asma A. and Bret, Am{\'e}lie and Cashin, Aidan G. and Ballou, Nick and Dumbalska, Tsvetomira and Kern, Bettina M. J. and Melia, Claire R. and Arendt, Beatrix and Vineyard, Gerald H. and Pickering, Jade S. and Evans, Thomas R. and Laverty, Catherine and Woodward, Eliza A. and Moreau, David and Roche, Dominique G. and Rinke, Eike M. and Reid, Graham and {Garcia-Garzon}, Eduardo and Verheyen, Steven and Kocalar, Halil E. and Blake, Ashley R. and Cockcroft, Jamie P. and Micheli, Leticia and Bret, Brice Beffara and Flack, Zoe M. and Szaszi, Barnabas and Weinmann, Markus and Lecuona, Oscar and Schmidt, Birgit and Ngiam, William X. and Mendes, Ana Barbosa and Francis, Shannon and Gall, Brett J. and Paul, Mariella and Keating, Connor T. and {Grose-Hodge}, Magdalena and Bartlett, James E. and Iley, Bethan J. and Spitzer, Lisa and Pownall, Madeleine and Graham, Christopher J. and Wingen, Tobias and Terry, Jenny and Oliveira, Catia Margarida F. and Millager, Ryan A. and Fox, Kerry J. and AlDoh, Alaa and Hart, Alexander and Van Den Akker, Olmo R. and Feldman, Gilad and Kiersz, Dominik A. and Pomareda, Christina and Krautter, Kai and {Al-Hoorie}, Ali H. and Aczel, Balazs},
  year = 2022,
  month = feb,
  journal = {Nature Human Behaviour},
  volume = {6},
  number = {3},
  pages = {312--318},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01269-4},
  urldate = {2025-11-06},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\ASVREIV4\Parsons et al. - 2022 - A community-sourced glossary of open scholarship terms.pdf}
}

@incollection{peirceTheoryProbableInference1883,
  title = {A Theory of Probable Inference},
  booktitle = {Studies in {{Logic}}},
  author = {Peirce, C. S.},
  year = 1883,
  pages = {126--181},
  publisher = {Little \& Brown},
  address = {Boston}
}

@book{penningtonStudentsGuideOpen2023,
  title = {A Student's Guide to Open Science: {{Using}} the Replication Crisis to Reform Psychology},
  author = {Pennington, C. R.},
  year = 2023,
  publisher = {McGraw-Hill Education},
  address = {UK}
}

@misc{penningtonStudyPreregistrationHowTo2024,
  title = {Study {{Preregistration}}: {{A How-To Guide}}},
  shorttitle = {Study {{Preregistration}}},
  author = {Pennington, Charlotte Rebecca},
  year = 2024,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/6g5zv},
  urldate = {2025-11-06},
  abstract = {Preregistration is the practice of formally documenting the plan for a study, including the research questions, hypotheses, design, and analysis before the data has been collected or analysed. To be a valid, this `study plan' should be prospectively registered in a publicly accessible, verified repository with an associated date- and timestamp. Preregistration can be implemented for all types of research, including quantitative, qualitative, and mixed methods, and primary and secondary data designs. Dependent on such methodology, the overarching goals of preregistration are to: (1) distinguish confirmatory from exploratory research; (2) increase transparency; (3) reduce bias; and (4) help evaluate the severity of hypothesis testing. Written for students, this guide provides an overview of the `what, why, and how' of preregistration as well as a discussion of its benefits, barriers, and solutions. After reading this guide, you should feel confident to use preregistration in your research training -- whether this be through adopting preregistration in your own research projects and/or being able to critically evaluate this open research practice in the published literature.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode}
}

@article{pridemoreReplicationCriminologySocial2018,
  title = {Replication in {{Criminology}} and the {{Social Sciences}}},
  author = {Pridemore, William Alex and Makel, Matthew C. and Plucker, Jonathan A.},
  year = 2018,
  journal = {Annual Review of Criminology},
  volume = {1},
  number = {Volume 1, 2018},
  pages = {19--38},
  publisher = {Annual Reviews},
  issn = {2572-4568},
  doi = {10.1146/annurev-criminol-032317-091849},
  urldate = {2025-11-19},
  abstract = {Replication is a hallmark of science. In recent years, some medical sciences and behavioral sciences struggled with what came to be known as replication crises. As a field, criminology has yet to address formally the threats to our evidence base that might be posed by large-scale and systematic replication attempts, although it is likely we would face challenges similar to those experienced by other disciplines. In this review, we outline the basics of replication, summarize reproducibility problems found in other fields, undertake an original analysis of the amount and nature of replication studies appearing in criminology journals, and consider how criminology can begin to assess more formally the robustness of our knowledge through encouraging a culture of replication.},
  langid = {english},
  file = {C\:\\Users\\Sarah von Grebmer\\Zotero\\storage\\6TWPFSIS\\Pridemore et al. - 2018 - Replication in Criminology and the Social Sciences.pdf;C\:\\Users\\Sarah von Grebmer\\Zotero\\storage\\4ZRC75IR\\annurev-criminol-032317-091849.html}
}

@article{prinzBelieveItNot2011,
  title = {Believe It or Not: How Much Can We Rely on Published Data on Potential Drug Targets?},
  shorttitle = {Believe It or Not},
  author = {Prinz, Florian and Schlange, Thomas and Asadullah, Khusru},
  year = 2011,
  journal = {Nature Reviews Drug Discovery},
  volume = {10},
  number = {9},
  pages = {712--712},
  publisher = {Nature Publishing Group},
  issn = {1474-1784},
  doi = {10.1038/nrd3439-c1},
  urldate = {2025-11-19},
  copyright = {2011 Springer Nature Limited},
  langid = {english},
  keywords = {Drug discovery},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\ZFWFPWUC\Prinz et al. - 2011 - Believe it or not how much can we rely on published data on potential drug targets.pdf}
}

@article{resnikScientificResearchPublic2011,
  title = {Scientific {{Research}} and the {{Public Trust}}},
  author = {Resnik, David B.},
  year = 2011,
  journal = {Science and Engineering Ethics},
  volume = {17},
  number = {3},
  pages = {399--409},
  issn = {1471-5546},
  doi = {10.1007/s11948-010-9210-x},
  urldate = {2025-11-20},
  abstract = {This essay analyzes the concept of public trust in science and offers some guidance for ethicists, scientists, and policymakers who use this idea defend ethical rules or policies pertaining to the conduct of research. While the notion that  public trusts science makes sense in the abstract, it may not be sufficiently focused to support the various rules and policies that authors have tried to derive from it, because the public is not a uniform body with a common set of interests. Well-focused arguments that use public trust to support rules or policies for the conduct of research should specify (a) which public is being referred to (e.g. the general public or a specific public, such as a particular community or group); (b) what this public expects from scientists; (c) how the rule or policy will ensure that these expectations are met; and (d) why is it important to meet these expectations.},
  langid = {english},
  keywords = {Ethics,Policy,Public expectations,Public trust,Science},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\T4S2WG97\Resnik - 2011 - Scientific Research and the Public Trust.pdf}
}

@article{roselerReplicationDatabaseDocumenting,
  title = {The {{Replication Database}}: {{Documenting}} the {{Replicability}} of {{Psychological Science}}},
  shorttitle = {The {{Replication Database}}},
  author = {R{\"o}seler, Lukas and Kaiser, Leonard and Doetsch, Christopher and Klett, Noah and Seida, Christian and Sch{\"u}tz, Astrid and Aczel, Balazs and Adelina, Nadia and Agostini, Valeria and Alarie, Samuel and {Albayrak-Aydemir}, Nihan and Aldoh, Alaa and {Al-Hoorie}, Ali H. and Azevedo, Flavio and Baker, Bradley J. and Barth, Charlotte Lilian and Beitner, Julia and Brick, Cameron and Brohmer, Hilmar and Chandrashekar, Subramanya Prasad and Chung, Kai Li and Cockcroft, Jamie P. and Cummins, Jamie and Diveica, Veronica and Dumbalska, Tsvetomira and Efendic, Emir and Elsherif, Mahmoud and Evans, Thomas and Feldman, Gilad and Fillon, Adrien and F{\"o}rster, Nico and Frese, Joris and Genschow, Oliver and Giannouli, Vaitsa and Gjoneska, Biljana and Gnambs, Timo and {Gourdon-Kanhukamwe}, Am{\'e}lie and Graham, Christopher J. and Hartmann, Helena and Haviva, Clove and Herderich, Alina and Hilbert, Leon P. and Holgado, Dar{\'i}as and Hussey, Ian and Ilchovska, Zlatomira G. and Kalandadze, Tamara and Karhulahti, Veli-Matti and Kasseckert, Leon and {Klingelh{\"o}fer-Jens}, Maren and Koppold, Alina and Korbmacher, Max and Kulke, Louisa and Kuper, Niclas and LaPlume, Annalise and Leech, Gavin and Lohkamp, Feline and Lou, Nigel Mantou and Lynott, Dermot and Maier, Maximilian and Meier, Maria and Montefinese, Maria and Moreau, David and Mrkva, Kellen and Nemcova, Monika and Oomen, Danna and Packheiser, Julian and Pandey, Shubham and Papenmeier, Frank and {Paruzel-Czachura}, Mariola and Pavlov, Yuri G. and Pavlovi{\'c}, Zoran and Pennington, Charlotte R. and Pittelkow, Merle-Marie and Plomp, Willemijn and Plonski, Paul E. and Pronizius, Ekaterina and Pua, Andrew Adrian and {Pypno-Blajda}, Katarzyna and Rausch, Manuel and Rebholz, Tobias R. and Richert, Elena and R{\"o}er, Jan Philipp and Ross, Robert and Schmidt, Kathleen and Skvortsova, Aleksandrina and Sperl, Matthias F. J. and Tan, Alvin W. M. and Th{\"u}rmer, J. Lukas and To{\l}opi{\l}o, Aleksandra and Vanpaemel, Wolf and Vaughn, Leigh Ann and Verheyen, Steven and Wallrich, Lukas and Weber, Lucia and Wolska, Julia K. and Zaneva, Mirela and Zhang, Yikang},
  journal = {Journal of Open Psychology Data},
  volume = {12},
  pages = {8},
  issn = {2050-9863},
  doi = {10.5334/jopd.101},
  urldate = {2025-11-19},
  abstract = {In psychological science, replicability---repeating a study with a new sample achieving consistent results ()---is critical for affirming the validity of scientific findings. Despite its importance, replication efforts are few and far between in psychological science with many attempts failing to corroborate past findings. This scarcity, compounded by the difficulty in accessing replication data, jeopardizes the efficient allocation of research resources and impedes scientific advancement. Addressing this crucial gap, we present the Replication Database (https://forrt-replications.shinyapps.io/fred\_explorer), a novel platform hosting 1,239 original findings paired with replication findings. The infrastructure of this database allows researchers to submit, access, and engage with replication findings. The database makes replications visible, easily findable via a graphical user interface, and tracks replication rates across various factors, such as publication year or journal. This will facilitate future efforts to evaluate the robustness of psychological research.},
  pmcid = {PMC12270267},
  pmid = {40687679},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\U724K98F\Röseler et al. - The Replication Database Documenting the Replicability of Psychological Science.pdf}
}

@article{rossiterMatthewMatildaEffect1993,
  title = {The {{Matthew Matilda Effect}} in {{Science}}},
  author = {Rossiter, Margaret W.},
  year = 1993,
  month = may,
  journal = {Social Studies of Science},
  volume = {23},
  number = {2},
  pages = {325--341},
  issn = {0306-3127, 1460-3659},
  doi = {10.1177/030631293023002004},
  urldate = {2025-09-25},
  abstract = {Recent work has brought to light so many cases, historical and contemporary, of women scientists who have been ignored, denied credit or otherwise dropped from sight that a sex-linked phenomenon seems to exist, as has been documented to be the case in other fields, such as medicine, art history and literary criticism. Since this systematic bias in scientific information and recognition practices fits the second half of Matthew 13:12 in the Bible, which refers to the under-recognition accorded to those who have little to start with, it is suggested that sociologists of science and knowledge can add to the 'Matthew Effect', made famous by Robert K. Merton in 1968, the 'Matilda Effect', named for the American suffragist and feminist critic Matilda J. Gage of New York, who in the late nineteenth century both experienced and articulated this phenomenon. Calling attention to her and this age-old tendency may prod future scholars to include other such 'Matildas' and thus to write a better, because more comprehensive, history and sociology of science.},
  copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
  langid = {english}
}

@article{rowsonCitationDiversityStatement2021,
  title = {Citation {{Diversity Statement}} in {{BMES Journals}}},
  author = {Rowson, Bethany and Duma, Stefan M. and King, Michael R. and Efimov, Igor and Saterbak, Ann and Chesler, Naomi C.},
  year = 2021,
  month = mar,
  journal = {Annals of Biomedical Engineering},
  volume = {49},
  number = {3},
  pages = {947--949},
  issn = {1573-9686},
  doi = {10.1007/s10439-021-02739-6},
  urldate = {2025-10-30},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\GZAUH9D9\Rowson et al. - 2021 - Citation Diversity Statement in BMES Journals.pdf}
}

@misc{sauvePursuitCitationalJustice2025,
  title = {In {{Pursuit}} of {{Citational Justice}}: {{A Toolkit}} for {{Equitable Scholarship}}},
  shorttitle = {In {{Pursuit}} of {{Citational Justice}}},
  author = {Sauv{\'e}, Sarah Anne and Middleton, Sara Lil and Gellersen, Helena and Azevedo, Flavio},
  year = 2025,
  publisher = {MetaArXiv},
  doi = {10.31222/osf.io/qjecy_v3},
  urldate = {2025-09-25},
  abstract = {Decades of empirical research across disciplines reveal the pervasiveness of citation biases along axes of gender, race, geography, and epistemology. Who we cite reflects and reinforces both the boundaries and hierarchies of academic knowledge, covertly shaping not only whose research is legitimized and valued but also whose careers are advanced. Citations, the currency of the academy with power to reinforce or dismantle hierarchies that privilege dominant knowledge systems cannot thus be a neutral, apolitical act. This paper unpacks the concept of citation politics and its role in sustaining epistemic hierarchies within scholarly communities. Rather than framing citation bias as a matter of individual oversight, we position it as a systemic issue tied to academic structures, norms, and incentive systems. We introduce a comprehensive and openly accessible Citational Justice Toolkit, developed by the FORRT community, which curates actionable resources, tools, and practices helping scholars and institutions to audit, diversify, and reflect on their citation practices across the research cycle. Our aim is to support a shift from tokenistic inclusion toward epistemically accountable, socially responsible, and structurally aware scholarship. We argue that reimagining citation as an ethical and epistemic practice is foundational to building a just and inclusive academic ecosystem, fostering pluralism, transparency, and integrity in knowledge production.},
  copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode}
}

@article{schmidtCreatingSPACEEvolve2021,
  title = {Creating {{SPACE}} to Evolve Academic Assessment},
  author = {Schmidt, Ruth and Curry, Stephen and Hatch, Anna},
  editor = {Rodgers, Peter and Beigel, Fernanda},
  year = 2021,
  journal = {eLife},
  volume = {10},
  pages = {e70929},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.70929},
  urldate = {2025-12-09},
  abstract = {Universities and research institutions have to assess individuals when making decisions about hiring, promotion and tenure, but there are concerns that such assessments are overly reliant on metrics and proxy measures of research quality that overlook important factors such as academic rigor, data sharing and mentoring. These concerns have led to calls for universities and institutions to reform the methods they use to assess research and researchers. Here we present a new tool called SPACE that has been designed to help universities and institutions implement such reforms. The tool focuses on five core capabilities and can be used by universities and institutions at all stages of reform process.},
  keywords = {academic assessment,DORA,evaluation,metrics,research assessment,research culture},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\EBEXFVTV\Schmidt et al. - 2021 - Creating SPACE to evolve academic assessment.pdf}
}

@article{shekarPeopleOvertrustAIGenerated2025,
  title = {People {{Overtrust AI-Generated Medical Advice}} despite {{Low Accuracy}}},
  author = {Shekar, Shruthi and Pataranutaporn, Pat and Sarabu, Chethan and Cecchi, Guillermo A. and Maes, Pattie},
  year = 2025,
  journal = {NEJM AI},
  volume = {2},
  number = {6},
  pages = {AIoa2300015},
  publisher = {Massachusetts Medical Society},
  doi = {10.1056/AIoa2300015},
  urldate = {2025-12-10}
}

@article{simmonsFalsePositivePsychologyUndisclosed2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = 2011,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797611417632},
  urldate = {2025-11-12},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\SKWNSUDU\Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility in Data Collection and Analysis Allows Presenting.pdf}
}

@article{smithWeAreNot2021,
  title = {``{{We}} Are Not Named'': {{Black}} Women and the Politics of Citation in Anthropology},
  shorttitle = {``{{We}} Are Not Named''},
  author = {Smith, Christen A. and Garrett-Scott, Dominique},
  year = 2021,
  month = may,
  journal = {Feminist Anthropology},
  volume = {2},
  number = {1},
  pages = {18--37},
  issn = {2643-7961, 2643-7961},
  doi = {10.1002/fea2.12038},
  urldate = {2025-09-25},
  abstract = {Abstract             Black women anthropologists are not cited within the discipline at a rate consistent with our scholarly production and visibility in the field. Despite our training, practice, and prolific writing, authors who publish in top-tier anthropology journals rarely cite Black women. This citational absence reveals a paradox: although Black women play key roles in the discipline as leaders and service providers, our intellectual contributions are undervalued. We are symbolically visible yet academically eclipsed. This article examines the epistemological erasure of Black women's contributions to anthropology in the United States. Through a pilot study, we measure Black women's citation rates in some of the highest ranked anthropology journals (according to impact factor). Moving away from a one-dimensional gender analysis toward a two-dimensional, intersectional analysis that analyzes race and gender, we find that Black women are underrepresented in citations in top-tier anthropology journals relative to their absolute representation in the field. This reveals a significant and disturbing trend: Black women anthropologists are rarely cited in top-tier anthropology journals, and in the rare instances they are cited, they are cited by other Black anthropologists. There is a need for an intersectional analysis of the politics of power and inequality in anthropology, one that not only pays attention to gender discrimination but also racial discrimination.},
  langid = {english}
}

@article{songPublicationBiasWhat2013,
  title = {Publication Bias: What Is It? {{How}} Do We Measure It? {{How}} Do We Avoid It?},
  shorttitle = {Publication Bias},
  author = {Song, Fujian and Hooper, Lee and Loke, Yoon K},
  year = 2013,
  journal = {Open Access Journal of Clinical Trials},
  volume = {5},
  pages = {71--81},
  issn = {null},
  doi = {10.2147/OAJCT.S34419},
  urldate = {2025-12-09},
  abstract = {Publication bias occurs when results of published studies are systematically different from results of unpublished studies. The term ``dissemination bias'' has also been recommended to describe all forms of biases in the research-dissemination process, including outcome-reporting bias, time-lag bias, gray-literature bias, full-publication bias, language bias, citation bias, and media-attention bias. We can measure publication bias by comparing the results of published and unpublished studies addressing the same question. Following up cohorts of studies from inception and comparing publication levels in studies with statistically significant or ``positive'' results suggested greater odds of formal publication in those with such results, compared to those without. Within reviews, funnel plots and related statistical methods can be used to indicate presence or absence of publication bias, although these can be unreliable in many circumstances. Methods of avoiding publication bias, by identifying and including unpublished outcomes and unpublished studies, are discussed and evaluated. These include searching without limiting by outcome, searching prospective trials registers, searching informal sources, including meeting abstracts and PhD theses, searching regulatory body websites, contacting authors of included studies, and contacting pharmaceutical or medical device companies for further studies. Adding unpublished studies often alters effect sizes, but may not always eliminate publication bias. The compulsory registration of all clinical trials at inception is an important move forward, but it can be difficult for reviewers to access data from unpublished studies located this way. Publication bias may be reduced by journals by publishing high-quality studies regardless of novelty or unexciting results, and by publishing protocols or full-study data sets. No single step can be relied upon to fully overcome the complex actions involved in publication bias, and a multipronged approach is required by researchers, patients, journal editors, peer reviewers, research sponsors, research ethics committees, and regulatory and legislation authorities.},
  keywords = {evidence synthesis,meta-analysis,publication bias,reporting bias,research-dissemination bias,systematic review},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\8TT3QPZE\Song et al. - 2013 - Publication bias what is it How do we measure it How do we avoid it.pdf}
}

@article{stefanBigLittleLies2023,
  title = {Big Little Lies: A Compendium and Simulation of {\emph{p}} -Hacking Strategies},
  shorttitle = {Big Little Lies},
  author = {Stefan, Angelika M. and Sch{\"o}nbrodt, Felix D.},
  year = 2023,
  journal = {Royal Society Open Science},
  volume = {10},
  number = {2},
  pages = {220346},
  issn = {2054-5703},
  doi = {10.1098/rsos.220346},
  urldate = {2025-10-30},
  abstract = {In many research fields, the widespread use of questionable research practices has jeopardized the credibility of scientific results. One of the most prominent questionable research practices is               p               -hacking. Typically,               p               -hacking is defined as a compound of strategies targeted at rendering non-significant hypothesis testing results significant. However, a comprehensive overview of these               p               -hacking strategies is missing, and current meta-scientific research often ignores the heterogeneity of strategies. Here, we compile a list of 12               p               -hacking strategies based on an extensive literature review, identify factors that control their level of severity, and demonstrate their impact on false-positive rates using simulation studies. We also use our simulation results to evaluate several approaches that have been proposed to mitigate the influence of questionable research practices. Our results show that investigating               p               -hacking at the level of strategies can provide a better understanding of the process of               p               -hacking, as well as a broader basis for developing effective countermeasures. By making our analyses available through a Shiny app and R package, we facilitate future meta-scientific research aimed at investigating the ramifications of               p               -hacking across multiple strategies, and we hope to start a broader discussion about different manifestations of               p               -hacking in practice.},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\I3HDH6Q2\Stefan and Schönbrodt - 2023 - Big little lies a compendium and simulation of p -hacking strategies.pdf}
}

@misc{stefanPreregistrationOpenScience2023,
  title = {Preregistration, {{Open Science Workshop}}},
  author = {Stefan, M., Angelika and Sch{\"o}nbrodt, Felix and Schiestel, Lena},
  year = 2023
}

@misc{stewartPreregistrationRegisteredReports2020,
  title = {Pre-Registration and {{Registered Reports}}: A {{Primer}} from {{UKRN}}},
  shorttitle = {Pre-Registration and {{Registered Reports}}},
  author = {Stewart, Suzanne and Rinke, Eike Mark and McGarrigle, Ronan and Lynott, Dermot and Lunny, Carole and Lautarescu, Alexandra and Galizzi, Matteo M and Farran, Emily Kate and Crook, Zander},
  year = 2020,
  month = oct,
  publisher = {Open Science Framework},
  doi = {10.31219/osf.io/8v2n7},
  urldate = {2025-11-06},
  abstract = {Help reduce questionable research practices, and prevent selective reporting.  This work is licensed under a Creative Commons Attribution 4.0 International License (CC BY).},
  archiveprefix = {Open Science Framework},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\58VSHTT2\Stewart et al. - 2020 - Pre-registration and Registered Reports a Primer from UKRN.pdf}
}

@article{sumnerGenderBalanceAssessment2018,
  title = {The {{Gender Balance Assessment Tool}} ({{GBAT}}): {{A Web-Based Tool}} for {{Estimating Gender Balance}} in {{Syllabi}} and {{Bibliographies}}},
  shorttitle = {The {{Gender Balance Assessment Tool}} ({{GBAT}})},
  author = {Sumner, Jane Lawrence},
  year = 2018,
  month = apr,
  journal = {PS: Political Science \& Politics},
  volume = {51},
  number = {02},
  pages = {396--400},
  issn = {1049-0965, 1537-5935},
  doi = {10.1017/S1049096517002074},
  urldate = {2025-09-25},
  abstract = {ABSTRACT             This article introduces a web-based tool that scholars can use to assess the gender balance of their syllabi and bibliographies. The citation gap in political science is described briefly as well as why under-citing women relative to men is a problem that should be addressed by the field. The Gender Balance Assessment Tool (GBAT) is presented as a way to make assessing gender balance easier with the aim of remedying the gender gap. This is followed by an outline that explains in nontechnical terms how the tool identifies author names and then predicts their gender to produce a single document-level percentage of women authors. Finally, best practices for diversity in syllabi and bibliographies are discussed, and various public sources that can be used to find scholarly work by women, as well as scholars of color, are listed.},
  langid = {english}
}

@misc{sumnerGenderBalanceAssessment2024,
  title = {Gender {{Balance Assessment Tool}} ({{GBAT}})},
  author = {Sumner, Jane Lawrence},
  year = 2024,
  urldate = {2025-09-25}
}

@article{turnerSelectivePublicationAntidepressant2008,
  title = {Selective Publication of Antidepressant Trials and {{Its}} Influence on Apparent Efficacy},
  author = {Turner, Erick H. and Matthews, Annette M. and Linardatos, Eftihia and Tell, Robert A. and Rosenthal, Robert},
  year = 2008,
  journal = {New England Journal of Medicine},
  volume = {358},
  number = {3},
  pages = {252--260},
  publisher = {Massachusetts Medical Society},
  issn = {0028-4793},
  doi = {10.1056/NEJMsa065779},
  urldate = {2025-12-09},
  abstract = {This study compares the published data on a dozen antidepressant drugs with analyses of the same drugs by the Food and Drug Administration. The results of studies in the entire database were less likely to be favorable to the drug than those in the published literature. This study compares the published data on a dozen antidepressant drugs with analyses of the same drugs by the FDA. The results of studies in the entire database were less likely to be favorable to the drug than those in the published literature. Medical decisions are based on an understanding of publicly reported clinical trials.1,2 If the evidence base is biased, then decisions based on this evidence may not be the optimal decisions. For example, selective publication of clinical trials, and the outcomes within those trials, can lead to unrealistic estimates of drug effectiveness and alter the apparent risk--benefit ratio.3,4 Attempts to study selective publication are complicated by the unavailability of data from unpublished trials. Researchers have found evidence for selective publication by comparing the results of published trials with information from surveys of authors,5 registries,6 institutional review boards,7,8 and .~.~.},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\EC7QTJ5D\Turner et al. - 2008 - Selective Publication of Antidepressant Trials and Its Influence on Apparent Efficacy.pdf}
}

@article{waldronNotAllPreregistrations2022,
  title = {Not All Pre-Registrations Are Equal},
  author = {Waldron, Sophie and Allen, Christopher},
  year = 2022,
  month = dec,
  journal = {Neuropsychopharmacology},
  volume = {47},
  number = {13},
  pages = {2181--2183},
  issn = {0893-133X, 1740-634X},
  doi = {10.1038/s41386-022-01418-x},
  urldate = {2025-11-06},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\QEJYLFLR\Waldron and Allen - 2022 - Not all pre-registrations are equal.pdf}
}

@article{wassersteinASAStatementPValues2016,
  title = {The {{ASA}} Statement on P-{{Values}}: {{Context}}, Process, and Purpose},
  shorttitle = {The {{ASA Statement}} on P-{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  year = 2016,
  journal = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133},
  publisher = {ASA Website},
  issn = {0003-1305},
  doi = {10.1080/00031305.2016.1154108},
  urldate = {2025-10-30},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\FMF2JKXL\Wasserstein and Lazar - 2016 - The ASA Statement on p-Values Context, Process, and Purpose.pdf}
}

@article{waterfieldDisabledAcademicsCase2018,
  title = {Disabled Academics: A Case Study in {{Canadian}} Universities},
  shorttitle = {Disabled Academics},
  author = {Waterfield, Bea and Beagan, Brenda B. and Weinberg, Merlinda},
  year = 2018,
  month = mar,
  journal = {Disability \& Society},
  volume = {33},
  number = {3},
  pages = {327--348},
  issn = {0968-7599, 1360-0508},
  doi = {10.1080/09687599.2017.1411251},
  urldate = {2025-09-25},
  langid = {english}
}

@article{wiseComputationalAccountThreatrelated2019,
  title = {A Computational Account of Threat-Related Attentional Bias},
  author = {Wise, Toby and Michely, Jochen and Dayan, Peter and Dolan, Raymond J.},
  editor = {Browning, Michael},
  year = 2019,
  month = oct,
  journal = {PLOS Computational Biology},
  volume = {15},
  number = {10},
  pages = {e1007341},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007341},
  urldate = {2025-11-06},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\XYIQ4V25\Wise et al. - 2019 - A computational account of threat-related attentional bias.pdf}
}

@article{wuGenderCitationGap2024,
  title = {The Gender Citation Gap: {{Approaches}}, Explanations, and Implications},
  shorttitle = {The Gender Citation Gap},
  author = {Wu, Cary},
  year = 2024,
  month = feb,
  journal = {Sociology Compass},
  volume = {18},
  number = {2},
  pages = {e13189},
  issn = {1751-9020, 1751-9020},
  doi = {10.1111/soc4.13189},
  urldate = {2025-09-25},
  abstract = {Abstract                            Do women face a disadvantage in terms of citation rates, and if so, in what ways? This article provides a comprehensive overview of existing research on the relationship between gender and citations. Three distinct approaches are identified: (1)               per-article               approach that compares gender differences in citations between articles authored by men and women, (2)               per-author               approach that compares the aggregate citation records of men and women scholars over a specified period or at the career level, and (3)               reference-ratio               approach that assesses the gender distribution of references in articles written by men and women. I show that articles written by women receive comparable or even higher rates of citations than articles written by men. However, women tend to accumulate fewer citations over time and at the career level. Contrary to the notion that women are cited less per article due to gender-based bias in research evaluation or citing behaviors, this study suggests that the primary reason for the lower citation rates at the author level is women publishing fewer articles over their careers. Understanding and addressing the gender citation gap at the author level should therefore focus on women's lower research productivity over time and the contributing factors. To conclude, I discuss the potential detrimental impact of lower citations on women's career progression and the ways to address the issue to mitigate gender inequalities in science.},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\4SVISFYC\Wu - 2024 - The gender citation gap Approaches, explanations, and implications.pdf}
}

@article{zurnCitationDiversityStatement2020,
  title = {The {{Citation Diversity Statement}}: {{A Practice}} of {{Transparency}}, {{A Way}} of {{Life}}},
  shorttitle = {The {{Citation Diversity Statement}}},
  author = {Zurn, Perry and Bassett, Danielle S. and Rust, Nicole C.},
  year = 2020,
  month = sep,
  journal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {9},
  pages = {669--672},
  issn = {13646613},
  doi = {10.1016/j.tics.2020.06.009},
  urldate = {2025-09-25},
  langid = {english},
  file = {C:\Users\Sarah von Grebmer\Zotero\storage\H666CSHZ\Zurn et al. - 2020 - The Citation Diversity Statement A Practice of Transparency, A Way of Life.pdf}
}
